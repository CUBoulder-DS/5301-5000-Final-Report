# Results {#sec-results}

```{r setup, include=FALSE}
#| include: false
# BJ
library(tidyverse)
library(caret)
library(kableExtra)
library(gt)
library(broom)
library(yardstick)
```

<!-- BJ !-->

```{r load_data, include=FALSE}
# CK, BJ
# master dataframe url
output_name <- 'https://raw.githubusercontent.com/CUBoulder-DS/5301-5000-Final-Report/main/data/MasterDataFrame.csv'

# entire master dataframe
output_df <- read_csv(output_name, show_col_types = FALSE) %>% 
  select(-c(img_path, file)) %>%
  mutate_if(is.character, as.factor) 

# Get chr columns as factors, re-ordered as they should
output_df$src_age_grp = factor(output_df$src_age_grp, 
                               levels = c("0-2", "3-9", "10-19", "20-29", "30-39", "40-49", "50-59", "60-69", "70-130"))
output_df$pred_age_grp = factor(output_df$pred_age_grp, 
                               levels = c("0-2", "3-9", "10-19", "20-29", "30-39", "40-49", "50-59", "60-69", "70-130"))
output_df$src_race = factor(output_df$src_race, levels=c("White", "Black", "Asian", "Indian", "Other"))
output_df$pred_race = factor(output_df$pred_race, levels=c("White", "Black", "Asian", "Indian", "Other"))

# Add correctness columns
output_df = output_df %>% 
  mutate(correct_gender = (src_gender == pred_gender), 
         correct_age = (src_age_grp == pred_age_grp), 
         correct_race = (src_race == pred_race))
  

# fairface dataframe
# exclude the indexing column and deepface only column
fairface_df <- output_df %>%
  filter(pred_model == 'FairFace') %>%
  select(-c(...1, pred_age_DF_only)) %>%
  drop_na()

# deepface dataframe
# exclude the indexing column
deepface_df <- output_df %>%
  filter(pred_model == 'DeepFace') %>%
  select(-...1) %>%
  drop_na()

# source dataframe
# use one of the models (since the data is doubled)
# include only the source ("src") columns
source_df <- output_df %>%
  filter(pred_model == 'FairFace') %>%
  select(c(src_age, src_age_grp, src_gender, src_race, src_timestamp)) %>%
  drop_na()

# p-value two sample calcs table
# df_name <- 'https://raw.githubusercontent.com/CUBoulder-DS/5301-5000-Final-Report/main/data/pvalue_results_two_sample.csv'
# p_value_results <- read_csv(df_name)

#added DeepFace only T-testing for age when controlled for gender/race
DeepFace_Only_T_tests <- read_csv('https://raw.githubusercontent.com/CUBoulder-DS/5301-5000-Final-Report/main/data/DF_t_tests.csv') %>% select(test_gender:power) %>% rename(gender=test_gender,race=test_race)

stats_results <- read_csv('https://raw.githubusercontent.com/CUBoulder-DS/5301-5000-Final-Report/main/data/combined_p_val_F1_acc.csv')

table_ages <- c("0-2", "3-9", "10-19", "20-29", "30-39", "40-49", "50-59", "60-69", "70-130")
table_races <- c("White", "Black", "Asian", "Indian", "Other")
table_genders <- c("Female", "Male")
```

<!-- BJ !-->

## Model Output

The two models, DeepFace and FairFace, were run on the dataset described previously. In @fig-output-hists, one can see the results of the predictions done by each model, by each factor that was considered: age, gender, and race. Note that the total (across correct and incorrect) histogram distributions match the correct (source dataset) distributions of values in each category, so we can see exactly the difference between what was provided and what was predicted, along with how well each model did on each category within each factor.

```{r plot_hists}
#| label: fig-output-hists
#| fig-cap: Histograms of the output from DeepFace and FairFace, with correct vs incorrect values colored. Note that the distributions match the correct (source dataset) distributions.
#| fig-subcap: 
#|   - Gender predictions
#|   - Age predictions
#|   - Race predictions
#| layout-ncol: 2
#| layout-nrow: 2
# BJ

plot_category = function(src, correct, label) {
  num_cats = length(levels(output_df[[src]]))
  
  plot = ggplot() +
    geom_bar(mapping=aes(x=as.numeric(interaction(output_df[["pred_model"]], output_df[[src]])),
                         fill=interaction(output_df[["pred_model"]], output_df[[correct]]))) +
    scale_x_continuous(label, 
                       breaks=seq(1.5, 2*num_cats, 2), 
                       labels=levels(output_df[[src]]), 
                       limits=c(0,2*num_cats + 1)) +
    scale_fill_manual("Model and Correctness",
                      values = c("orangered", "firebrick", "springgreen", "green4"),
                      labels = c("DeepFace, incorrect", "FairFace, incorrect", "DeepFace, correct", "FairFace, correct"))
  
  return(plot)
}


plot_category("src_gender", "correct_gender", "Gender")
plot_category("src_age_grp", "correct_age", "Age")
plot_category("src_race", "correct_race", "Race")
```

## Model Performance, Hypothesis Testing

```{r view_stat_results}
# BJ
view_results = stats_results %>% 
                filter(test_age == "All" | test_gender == "All" | test_race == "All") %>%
                select(-c(source_n, model_n, model_prop, source_prop)) %>% 
                replace(. == "age_bins", "age") %>%
                replace(. == "genders", "gender") %>%
                replace(. == "races", "race")
```

For each factor category and model, we calculate the F1 score, accuracy, p-value, and power, as described in section 3. Cell values are colored according to the strength of the metric; p-value is colored as to whether it crosses the significance value threshold of 0.003. We calculate these metrics and hypothesis tests across all categories of each factor, but also with conditional filtering on other factors; the value "All" indicates we did not filter/condition on that factor. The column `Test Factor` indicates which factor we are calculating the proportion for that hypothesis test. For example, the following column value subsets would indicate the given hypothesis test:

| **Test Factor** | **Age** | **Gender** | **Race** | Model    | Null Hypothesis                      | Description                                                                                  |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| gender          | 0-2     | Female     | All      | FairFace | $p_{F, D_f | A_1} = p_{F,D_0 | A_1}$ | $H_0$ : The proportions of Female labels, given that the source age label is 0-2, are equal. |
| race            | All     | All        | Black    | DeepFace | $p_{R_B, D_d} = p_{R_B, D_0}$        | $H_0$: The proportions of Black labels are equal.                                            |

::: {.content-visible when-format="html"}
The results are summarized in @tbl-perf-pvalue, which is interactive and filterable.

<!-- ::: {#tbl-perf-pvalue} -->

```{r perf-pvalue}
#| label: tbl-perf-pvalue
#| tbl-cap: Table of F1 score, accuracy, p-value, and power, by each factor and category evaluated by the models, with a potential filtering condition.
# BJ
# Note: For some reason, making the table interactive (aka HTML) breaks Quarto table labeling/captioning/crossref
# That's why we use a combination of table div and label.

colnames = setNames(c("**Test Factor**", "**Age**", "**Gender**", "**Race**", "**p-Value**", "**Power**", "**F1 Score**", "**Accuracy**", "**Model**"),
                    names(view_results))

view_results %>%
  gt() %>%
  opt_interactive(use_search = T, use_filters = T, use_highlight = T, page_size_default = 15, use_resizers=T) %>%
  tab_options(table.width = pct(100), quarto.use_bootstrap=T) %>%
  fmt_scientific(columns = 5, n_sigfig=3, drop_trailing_zeros=T, exp_style="e") %>%
  fmt_number(columns = 6:8, decimal=4) %>%
  cols_label(.list=as.list(colnames), .fn=md) %>%
  data_color(columns=6:8, palette="PiYG", domain=c(0, 1), alpha=0.75) %>%
  data_color(columns=5, method="bin", palette = c("#B1DC96", "#F2B8D2"), bins=c(0, 0.003, 1))
```

<!-- ::: -->
:::

::: {.content-visible when-format="pdf"}
The results are summarized in @fig-perf-pvalue-pdf.

![Screenshot of the interactive table showing TODO. To see and interact with this table, go to [the website link](https://cuboulder-ds.github.io/5301-5000-Final-Report/results.html)](%22images/table_interactive.png%22){#fig-perf-pvalue-pdf}
:::

### p-value Critical Values

```{r calc-criticals}

cats = c("age", "gender", "race")
pvalue_thresh = 0.003
power_thresh = 0.8
f1_thresh = 0.9

# Because R is stupid about concatenating strings
t_str = function(x) {
  paste("test", x, sep="_")
}

get_hyp_results = function(full_df, reject_null, noconds=TRUE) {
  if (reject_null) {
      filtered = full_df %>% 
                  filter(model_p_value < pvalue_thresh,
                         model_power >= power_thresh,
                         model_F1 < f1_thresh | is.na(model_F1)
                         )
  } else {
      filtered = full_df %>% 
                  filter(model_p_value >= pvalue_thresh,
                         model_power < power_thresh,
                         model_F1 >= f1_thresh
                         )
  }

  if (noconds) {
    return(filtered %>%
            filter((test_age == "All") + (test_gender == "All") + (test_race == "All") > 1))
    
  } else return(filtered)
}

get_opposite_rows = function(row, full_df, reject_null) {
  other_cats = cats[cats != row["test_prop"]]
  
  full_df %>%
      filter(test_prop == row["test_prop"],
             # Get same test prop category val and model 
             .[[t_str(row["test_prop"])]] == row[t_str(row["test_prop"])],
             model == row["model"],
             # Get row where there is some test filtering sub condition
             .[[t_str(other_cats[1])]] != "All" | .[[t_str(other_cats[2])]] != "All") %>%
      # Get rows with the opposite hypothesis rejection result
      get_hyp_results(reject_null=reject_null, noconds=FALSE)
}


reject_nulls =  view_results %>% get_hyp_results(T)
reject_nulls_opp = bind_rows(apply(reject_nulls, 1, get_opposite_rows, full_df=view_results, reject_null=F))

reject_nulls_ff = reject_nulls %>% filter(model == "FairFace")
reject_nulls_df = reject_nulls %>% filter(model == "DeepFace")

reject_nulls_opp_ff = reject_nulls_opp %>% filter(model == "FairFace")
reject_nulls_opp_df = reject_nulls_opp %>% filter(model == "DeepFace")

no_reject_nulls = view_results %>% get_hyp_results(F)
no_reject_nulls_opp = bind_rows(apply(no_reject_nulls, 1, get_opposite_rows, full_df=view_results, reject_null=T))
```

From the previous table, we extract and highlight key values; namely, where we reject the null hypothesis and where we do not, based on our criteria:

-   Significance level of 99.7%
-   Power threshold of 0.8
-   F1-Score of 0.9

Which come from the rationale described in @sec-methods. We show the test values where there is no sub-filtering/conditions by another category; then, we also highlight the reverse null hypothesis decisions made with filtering for a sub-condition and for the specific rows as described in the table captions. The coloring of cells is the same as in @tbl-perf-pvalue. The values are displayed in @tbl-reject-nulls. There is only a Fairface table for not rejecting the null hypothesis (with no condition subfiltering) because no DeepFace values passed our given thresholds for not rejecting; the same reasoning is why there is no table for FairFace rejecting the null hypothesis with condition subfiltering.

```{r reject-nulls}
#| label: tbl-reject-nulls
#| tbl-cap: Highlighted statistics/metrics for DeepFace and FairFace, that pass the given significance level/power/F1-score thresholding. 
#| tbl-subcap: 
#|   - "FairFace: Reject the null hypothesis, with no condition subfiltering"
#|   - "FairFace: Do not reject the null hypothesis, for the factor category given in (a), for subcondition filtering"
#|   - "DeepFace: Reject the null hypothesis, with no condition subfiltering"
#|   - "DeepFace: Do not reject the null hypothesis, for the factor category given in (c), for subcondition filtering"
#|   - "FairFace: Do not reject the null hypothesis,  with no condition subfiltering"
#| layout-ncol: 2
# BJ


no_conds_cols = setNames(c("Test Factor", "Category", "p-Value", "Power", "F1 Score"),
                          c("test_prop", "test_subcat", "model_p_value", "model_power", "model_F1"))
conds_cols = setNames(c("Test Factor", "Age", "Gender", "Race", "p-Value", "Power", "F1 Score"),
                          c("test_prop", "test_age", "test_gender", "test_race", "model_p_value", "model_power", "model_F1"))

fmt_gt = function(x) {
  x %>%
    select(-c(model_Accuracy, model)) %>%
    group_by(test_prop) %>%
    gt() %>%
    tab_options(column_labels.font.weight="bold",
                row_group.font.weight="bold",
                row_group.as_column = T
                ) %>%
    # Format the p-value column
    fmt_scientific(columns = model_p_value, n_sigfig=3, drop_trailing_zeros=T, exp_style="e") %>%
    ## BJ: Colors don't work in Latex!!!!!
    # data_color(columns=model_p_value, method="bin", palette = c("#B1DC96", "#F2B8D2"), bins=c(0, 0.003, 1)) %>%
    # Format the other number columns
    fmt_number(columns = model_power:model_F1, decimal=4)
    # data_color(columns=model_power:model_F1, palette="PiYG", domain=c(0, 1), alpha=0.75)
}


reject_nulls_ff %>%
  replace(. == "All", NA) %>%
  unite("test_subcat", test_age:test_race, remove=T, na.rm=T) %>%
  fmt_gt() %>%
  cols_label(.list=no_conds_cols)

reject_nulls_opp_ff %>%
  fmt_gt() %>%
  cols_label(.list=conds_cols)

reject_nulls_df %>%
  replace(. == "All", NA) %>%
  unite("test_subcat", test_age:test_race, remove=T, na.rm=T) %>%
  fmt_gt() %>%
  cols_label(.list=no_conds_cols)

reject_nulls_opp_df %>%
  fmt_gt() %>%
  cols_label(.list=conds_cols)

no_reject_nulls %>%
  replace(. == "All", NA) %>%
  unite("test_subcat", test_age:test_race, remove=T, na.rm=T) %>%
  fmt_gt() %>%
  cols_label(.list=no_conds_cols)
```

## Meta-Analysis Plots

In @fig-f1-acc, we show F1-score vs accuracy for all hypothesis tests that were performed. Note the relationship is not perfectly linear.

```{r, warning=F}
#| label: fig-f1-acc
#| fig-cap: F1-Score vs Accuracy for all hypothesis tests performed.
# BJ

ggplot(stats_results) +
  geom_point(aes(x=model_F1, y=model_Accuracy, color=model)) +
  labs(x="F1 Score", y="Accuracy")
```

```{r, warning=F}
# BJ

#PC - commented out, we decided this plot didn't make sense.
# ggplot(stats_results) +
#   geom_point(aes(x=model_F1, y=model_p_value, color=model, fill=model_power), shape=21)

# ggplot(stats_results) +
#   geom_point(aes(x=model_F1, y=model_p_value, color=model))


#classify our test results as TP,TN,FP,FN or unknown.
classify <- function(p_val,power,F1){
  #handle NAs...
	if (!is.numeric(p_val) | !is.numeric(F1) | is.nan(p_val) | is.nan(F1)){
			"Unknown"
  #assume cases where we have no F1 or P-value
	} else if (is.na(F1) | is.na(p_val)) {
    if (!is.na(p_val)&p_val < 0.003){
		"True Positive"  #TP = reject the null, and we should
    } else {
      "Unknown"
    }
	} else if(p_val >=0.003 & F1 >= 0.9 & power < 0.8){
		"True Negative" #TN = fail to reject the null when we shouldn't
	} else if (p_val < 0.003 & F1 < 0.9 & power >= 0.8) {
		"True Positive" #TP = reject the null, and we should
	} else if (p_val >= 0.003 & F1 < 0.9 & power < 0.8) {
		"False Negative" #FN = fail to reject the null when it's false 
	} else if (p_val < 0.003 & F1 > 0.9 & power >= 0.8) {
		"False Positive" #FP = reject the null when it's true 
	} else {
		"Unknown" #not sure how we should classify the case.
	}
}

test_result <- function(pval){
  if(is.na(pval) | is.nan(pval)){
    "Unknown"
  } else if (pval<.003){
    "Reject null"
  } else {
    "Fail to reject null"
  }
}

result_confusion <- function(p_val,power,F1){
    #handle NAs...
	if (!is.numeric(p_val) | !is.numeric(F1) | is.nan(p_val) | is.nan(F1)){
			"Unknown"
  #assume cases where we have no F1 or P-value
	} else if (is.na(F1) | is.na(p_val)) {
    if (!is.na(p_val)&p_val < 0.003){
		"Reject null"  #TP = reject the null, and we should
    } else {
      "Unknown"
    }
	} else if(p_val >=0.003 & F1 >= 0.9 & power < 0.8){
		"Fail to reject null" #TN = fail to reject the null when we shouldn't
	} else if (p_val < 0.003 & F1 < 0.9 & power >= 0.8) {
		"Reject null" #TP = reject the null, and we should
	} else if (p_val >= 0.003 & F1 < 0.9 & power < 0.8) {
		"Reject null" #FN = fail to reject the null when it's false 
	} else if (p_val < 0.003 & F1 > 0.9 & power >= 0.8) {
		"Fail to reject null" #FP = reject the null when it's true 
	} else {
		"Unknown" #not sure how we should classify the case.
	}
}

categorized_results <- stats_results %>% 
  rowwise %>%
  mutate(
    result_category=classify(model_p_value,model_power,model_F1),
    p_value_classification=test_result(model_p_value),
    f1_classification=result_confusion(model_p_value,model_power,model_F1)
  ) %>%
  mutate(
    F1_pass = as.numeric(model_F1>=.9),
    reject_null = as.numeric(model_p_value < 0.003 & model_power > 0.8)
  )

# ###plot the graph as jitter points
# categorized_results %>% filter(model=='DeepFace') %>%
#   ggplot()+
#     geom_jitter(aes(x=reject_null,y=F1_pass,color=result_category))
# 
# categorized_results %>% filter(model=='FairFace') %>%
#   ggplot()+
#     geom_jitter(aes(x=reject_null,y=F1_pass,color=result_category))
# 
# 
# #try to make confusion matrices and turn them into tile charts.
# 
fac_lev <- categorized_results %>% pull(f1_classification) %>% unique()
plot_cm <- categorized_results %>%
  mutate(
    p_value_classification=factor(p_value_classification,levels=fac_lev),
    f1_classification=factor(f1_classification,levels=fac_lev)
  )
# 
ff_tests <- plot_cm %>% filter(model=='FairFace')
# ff_results_conf <- confusionMatrix(data=ff_tests$p_value_classification,reference=ff_tests$f1_classification)
df_tests <- plot_cm %>% filter(model=='DeepFace')
# df_results_conf <- confusionMatrix(data=df_tests$p_value_classification,reference=df_tests$f1_classification,)$table


```

In @fig-p-conf, we display confusion matrices of our null hypothesis rejections. We define the true/false positive/negative as follows:

-   Reject null when we should reject null: p-value \< 0.003, F1 \< 0.9, power \>= 0.8
-   Reject null when we should fail to reject null: p-value \< 0.003, F1 \> 0.9, power \>= 0.8
-   Fail to reject null, when we should reject null: p-value \>= 0.003, F1 \< 0.9, power \< 0.8
-   Fail to reject null, when we should fail to reject null: p-value \>=0.003, F1 \>= 0.9, power \< 0.8
-   Uknown: One of the values was NaN.

```{r, warning=F}
#| label: fig-p-conf
#| fig-cap: Confusion matrices of null rejection decisions.
#| fig-subcap: 
#|   - Matrix for FairFace
#|   - Matrix for DeepFace
#| layout-ncol: 2

# BJ
autoplot(conf_mat(ff_tests, p_value_classification, f1_classification), type = "heatmap") +
  scale_fill_distiller(palette = "YlGn", direction = 1) +
  labs(title="FairFace")

autoplot(conf_mat(df_tests, p_value_classification, f1_classification), type = "heatmap") +
  scale_fill_distiller(palette = "YlGn", direction = 1) +
  labs(title="DeepFace")
```

<!-- ### Statistical Power - TODO REMOVE -->

```{=html}
<!-- $$
\beta = P\bigg(\bigg|\frac{\sqrt{n}\cdot\hat{p}-p_a}{\sqrt{p_a(1-p_a)}}\bigg|\geq\frac{\sqrt{n}\cdot p_0-p_a}{\sqrt{p_0(1-p_0)}}\bigg)
$$ -->
```
```{=html}
<!-- Our selected level of significance is 99.7% (3-sigma). Type-II error is denoted by $\beta$ above, and Power will be $1-\beta$

With $p_0$ being our *assumed* population proportion (from the source dataset and what we used in our tests), $p_a$ being the *actual* population proportion (from one or more of the below methods), $n$ being the number of predicted members of a racial group (i.e. "Indian"),

-   For Gender - assume that sex at birth is a bernoulli trial, over time, the proportion for both genders should be 0.5

-   For age groups - assume that age has a true normal distribution. Each race may have different means and standard deviations for their distribution of age, but still adhere to a normal distribution. The "population" proportions may be a bit more challenging to calculate, but under this framework, we may be able to get there.

    -   May be able to get via bootstrapping the source dataset, average age by race - I think that's what we did in our last project?

    -   Could look at external data? May not have time to look through everything. -->
```
```{r PowerCalcs}
#can be removed.
# TypeII <- function(p0,pa,n){
#   z <- (sqrt(n)*(p0-pa))/(sqrt(p0*(1-p0)))
# }

```
