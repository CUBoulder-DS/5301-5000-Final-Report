# Results

```{r setup, include=FALSE}
#| include: false
# BJ
library(tidyverse)
library(caret)
library(kableExtra)
library(gt)
library(broom)
library(yardstick)
```

<!-- BJ !-->

```{r load_data, include=FALSE}
# CK, BJ
# master dataframe url
output_name <- 'https://raw.githubusercontent.com/CUBoulder-DS/5301-5000-Final-Report/main/data/MasterDataFrame.csv'

# entire master dataframe
output_df <- read_csv(output_name, show_col_types = FALSE) %>% 
  select(-c(img_path, file)) %>%
  mutate_if(is.character, as.factor) 

# Get chr columns as factors, re-ordered as they should
output_df$src_age_grp = factor(output_df$src_age_grp, 
                               levels = c("0-2", "3-9", "10-19", "20-29", "30-39", "40-49", "50-59", "60-69", "70-130"))
output_df$pred_age_grp = factor(output_df$pred_age_grp, 
                               levels = c("0-2", "3-9", "10-19", "20-29", "30-39", "40-49", "50-59", "60-69", "70-130"))
output_df$src_race = factor(output_df$src_race, levels=c("White", "Black", "Asian", "Indian", "Other"))
output_df$pred_race = factor(output_df$pred_race, levels=c("White", "Black", "Asian", "Indian", "Other"))

# Add correctness columns
output_df = output_df %>% 
  mutate(correct_gender = (src_gender == pred_gender), 
         correct_age = (src_age_grp == pred_age_grp), 
         correct_race = (src_race == pred_race))
  

# fairface dataframe
# exclude the indexing column and deepface only column
fairface_df <- output_df %>%
  filter(pred_model == 'FairFace') %>%
  select(-c(...1, pred_age_DF_only)) %>%
  drop_na()

# deepface dataframe
# exclude the indexing column
deepface_df <- output_df %>%
  filter(pred_model == 'DeepFace') %>%
  select(-...1) %>%
  drop_na()

# source dataframe
# use one of the models (since the data is doubled)
# include only the source ("src") columns
source_df <- output_df %>%
  filter(pred_model == 'FairFace') %>%
  select(c(src_age, src_age_grp, src_gender, src_race, src_timestamp)) %>%
  drop_na()

# p-value two sample calcs table
# df_name <- 'https://raw.githubusercontent.com/CUBoulder-DS/5301-5000-Final-Report/main/data/pvalue_results_two_sample.csv'
# p_value_results <- read_csv(df_name)

#added DeepFace only T-testing for age when controlled for gender/race
DeepFace_Only_T_tests <- read_csv('https://raw.githubusercontent.com/CUBoulder-DS/5301-5000-Final-Report/main/data/DF_t_tests.csv') %>% select(test_gender:power) %>% rename(gender=test_gender,race=test_race)

stats_results <- read_csv('https://raw.githubusercontent.com/CUBoulder-DS/5301-5000-Final-Report/main/data/combined_p_val_F1_acc.csv')

table_ages <- c("0-2", "3-9", "10-19", "20-29", "30-39", "40-49", "50-59", "60-69", "70-130")
table_races <- c("White", "Black", "Asian", "Indian", "Other")
table_genders <- c("Female", "Male")
```

<!-- BJ !-->

## Model Output

The two models, DeepFace and FairFace, were run on the dataset described previously. In @fig-output-hists, one can see the results of the predictions done by each model, by each factor that was considered: age, gender, and race. Note that the total (across correct and incorrect) histogram distributions match the correct (source dataset) distributions of values in each category, so we can see exactly the difference between what was provided and what was predicted, along with how well each model did on each category within each factor.

```{r plot_hists}
#| label: fig-output-hists
#| fig-cap: Histograms of the output from DeepFace and FairFace, with correct vs incorrect values colored. Note that the distributions match the correct (source dataset) distributions.
#| fig-subcap: 
#|   - Gender predictions
#|   - Age predictions
#|   - Race predictions
#| layout-ncol: 2
#| layout-nrow: 2
# BJ

plot_category = function(src, correct, label) {
  num_cats = length(levels(output_df[[src]]))
  
  plot = ggplot() +
    geom_bar(mapping=aes(x=as.numeric(interaction(output_df[["pred_model"]], output_df[[src]])),
                         fill=interaction(output_df[["pred_model"]], output_df[[correct]]))) +
    scale_x_continuous(label, 
                       breaks=seq(1.5, 2*num_cats, 2), 
                       labels=levels(output_df[[src]]), 
                       limits=c(0,2*num_cats + 1)) +
    scale_fill_manual("Model and Correctness",
                      values = c("orangered", "firebrick", "springgreen", "green4"),
                      labels = c("DeepFace, incorrect", "FairFace, incorrect", "DeepFace, correct", "FairFace, correct"))
  
  return(plot)
}


plot_category("src_gender", "correct_gender", "Gender")
plot_category("src_age_grp", "correct_age", "Age")
plot_category("src_race", "correct_race", "Race")
```


## Model Performance, Hypothesis Testing

```{r view_stat_results}
# BJ
view_results = stats_results %>% 
                filter(test_age == "All" | test_gender == "All" | test_race == "All") %>%
                select(-c(source_n, model_n, model_prop, source_prop)) %>% 
                replace(. == "age_bins", "ages")
```

For each category and model, we calculate the F1 score, accuracy, and p-value, as described in section 3. Cell values are colored according to the strength of the metric. We also specifically looked at the performance metrics of the models, when controlled for specific source categories; TODO

### TODO

-   Add color key
-   Better description of signifiance of numerical values of Accuracy, F1 score, power

::: {.content-visible when-format="html"}
The results are summarized in @tbl-perf-pvalue. 

::: {#tbl-perf-pvalue}
```{r perf-pvalue}
#| label: tbl-perf-pvalue
# BJ
# Note: For some reason, making the table interactive (aka HTML) breaks Quarto table labeling/captioning/crossref
# That's why we use a combination of table div and label.

colnames = setNames(c("**Test Factor**", "**Age**", "**Gender**", "**Race**", "**p-Value**", "**Power**", "**F1 Score**", "**Accuracy**", "**Model**"),
                    names(view_results))

view_results %>%
  gt() %>%
  opt_interactive(use_search = T, use_filters = T, use_highlight = T, page_size_default = 15, use_resizers=T) %>%
  tab_options(table.width = pct(100), quarto.use_bootstrap=T) %>%
  fmt_scientific(columns = 5, n_sigfig=3, drop_trailing_zeros=T, exp_style="e") %>%
  fmt_number(columns = 6:8, decimal=4) %>%
  cols_label(.list=as.list(colnames), .fn=md) %>%
  data_color(columns=6:8, palette="PiYG", domain=c(0, 1), alpha=0.75) %>%
  data_color(columns=5, method="bin", palette = c("#B1DC96", "#F2B8D2"), bins=c(0, 0.003, 1))
```


TODO 3 Table of F1 score, accuracy, p-value, and power, by each category evaluated by the models
:::
:::

::: {.content-visible when-format="pdf"}
The results are summarized in @fig-perf-pvalue-pdf. 

![Screenshot of the interactive table showing TODO. To see and interact with this table, go to [the website link](https://cuboulder-ds.github.io/5301-5000-Final-Report/results.html)]("images/table_interactive.png"){#fig-perf-pvalue-pdf}
:::

### p-value Critical Values

From the previous table, we extract and highlight key values; namely, the p-values where we reject the null hypothesis at a significance level of 99.7%, and where there is no conditional filtering based on another category. The values are displayed in @tbl-perf-selected.

```{r tbl-perf-selected}
#| label: tbl-perf-selected
#| tbl-cap: TODO 
# BJ

view_results %>%
  filter(model_p_value < 0.003, 
         model_power > 0.8, 
         (test_age == "All") + (test_gender == "All") + (test_race == "All") > 1) %>%
  replace(. == "All", NA) %>%
  unite("test_subcat", test_age:test_race, remove=T, na.rm=T) %>%
  gt()

# tbl_gt = combined_results %>% 
#   filter(p_value_f > 0.03 | p_value_d > 0.03) %>%
#   select(Race, Category, p_value_d, p_value_f) %>%
#   unite(cat, c(Race, Category)) %>%
#   # t() %>% data.frame() %>% row_to_names(1) %>%
#   # add_column("Model p-value" = c("DeepFace", "FairFace"), .before=1) %>%
#   gt(rowname_col = "cat") %>%
#   tab_options(column_labels.font.weight="bold", 
#           row_group.font.weight="bold",
#           # stub.font.weight="bold",
#           # table.width = pct(100), 
#           # TODO: DOESN'T WORK?!?!!
#           # row_group.as_column = T
#           ) %>%
#   fmt_number(decimals=4) %>%
#   tab_stubhead("Test Condition, Category") %>%
#   text_transform( \(x) str_remove(x, ".*_"), locations = cells_stub()) %>%
#   cols_label(p_value_d="DeepFace", p_value_f="FairFace") %>%
#   tab_spanner("p-value", columns=1:3) %>%
#   data_color(method="bin", palette = c("#B1DC96", "#F2B8D2"), bins=c(0, 0.03, 1)) %>%
#   tab_row_group("No Test Condition", rows=starts_with("All"), id="All")
# for (r in table_races) {
#   tbl_gt = tab_row_group(tbl_gt, sprintf("Race: %s", r), rows=starts_with(r), id=r)
# }
# 
# # tbl_gt = gt_split(tbl_gt, row_every_n=5)
# 
# tbl_gt
```

```{r tbl-DF-T-Tests, warning=F}
#| label: tbl-DF-T-Tests
#| tbl-cap: Rejected Null Hypotheses for DeepFace Mean Age T-Testing

#PC
##ran t tests to compare mean ages between source data and DeepFace
# DeepFace_Only_T_tests %>%
#   filter(p_value <0.003,power > 0.8) %>%
#   gt()

# T-Test Parameters: 

# * Confidence level: 0.997

# * Two-sided test

# Statistical Power T-Test Parameters: 

# * Effect Size: +/- 5 years in age

# * Significance level: 0.003

# * Two-sided test
```


### Comparison Plots

```{r, warning=F}
ggplot(stats_results) +
  geom_point(aes(x=model_F1, y=model_Accuracy, color=model)) +
  labs(x="F1 Score", y="Accuracy")
```


```{r, warning=F}
# BJ

#PC - commented out, we decided this plot didn't make sense.
# ggplot(stats_results) +
#   geom_point(aes(x=model_F1, y=model_p_value, color=model, fill=model_power), shape=21)



# ggplot(stats_results) +
#   geom_point(aes(x=model_power, y=model_p_value, color=model))


#classify our test results as TP,TN,FP,FN or unknown.
classify <- function(p_val,power,F1){
  #handle NAs...
	if (!is.numeric(p_val) | !is.numeric(F1) | is.nan(p_val) | is.nan(F1)){
			"Unknown"
  #assume cases where we have no F1 or P-value
	} else if (is.na(F1) | is.na(p_val)) {
    if (!is.na(p_val)&p_val < 0.003){
		"True Positive"  #TP = reject the null, and we should
    } else {
      "Unknown"
    }
	} else if(p_val >=0.003 & F1 >= 0.9 & power < 0.8){
		"True Negative" #TN = fail to reject the null when we shouldn't
	} else if (p_val < 0.003 & F1 < 0.9 & power >= 0.8) {
		"True Positive" #TP = reject the null, and we should
	} else if (p_val >= 0.003 & F1 < 0.9 & power < 0.8) {
		"False Negative" #FN = fail to reject the null when it's false 
	} else if (p_val < 0.003 & F1 > 0.9 & power >= 0.8) {
		"False Positive" #FP = reject the null when it's true 
	} else {
		"Unknown" #not sure how we should classify the case.
	}
}

test_result <- function(pval){
  if(is.na(pval) | is.nan(pval)){
    "Unknown"
  } else if (pval<.003){
    "Reject null"
  } else {
    "Fail to reject null"
  }
}

result_confusion <- function(p_val,power,F1){
    #handle NAs...
	if (!is.numeric(p_val) | !is.numeric(F1) | is.nan(p_val) | is.nan(F1)){
			"Unknown"
  #assume cases where we have no F1 or P-value
	} else if (is.na(F1) | is.na(p_val)) {
    if (!is.na(p_val)&p_val < 0.003){
		"Reject null"  #TP = reject the null, and we should
    } else {
      "Unknown"
    }
	} else if(p_val >=0.003 & F1 >= 0.9 & power < 0.8){
		"Fail to reject null" #TN = fail to reject the null when we shouldn't
	} else if (p_val < 0.003 & F1 < 0.9 & power >= 0.8) {
		"Reject null" #TP = reject the null, and we should
	} else if (p_val >= 0.003 & F1 < 0.9 & power < 0.8) {
		"Reject null" #FN = fail to reject the null when it's false 
	} else if (p_val < 0.003 & F1 > 0.9 & power >= 0.8) {
		"Fail to reject null" #FP = reject the null when it's true 
	} else {
		"Unknown" #not sure how we should classify the case.
	}
}

categorized_results <- stats_results %>% 
  rowwise %>%
  mutate(
    result_category=classify(model_p_value,model_power,model_F1),
    p_value_classification=test_result(model_p_value),
    f1_classification=result_confusion(model_p_value,model_power,model_F1)
  ) %>%
  mutate(
    F1_pass = as.numeric(model_F1>=.9),
    reject_null = as.numeric(model_p_value < 0.003 & model_power > 0.8)
  )

# ###plot the graph as jitter points
# categorized_results %>% filter(model=='DeepFace') %>%
#   ggplot()+
#     geom_jitter(aes(x=reject_null,y=F1_pass,color=result_category))
# 
# categorized_results %>% filter(model=='FairFace') %>%
#   ggplot()+
#     geom_jitter(aes(x=reject_null,y=F1_pass,color=result_category))
# 
# 
# #try to make confusion matrices and turn them into tile charts.
# 
fac_lev <- categorized_results %>% pull(f1_classification) %>% unique()
plot_cm <- categorized_results %>%
  mutate(
    p_value_classification=factor(p_value_classification,levels=fac_lev),
    f1_classification=factor(f1_classification,levels=fac_lev)
  )
# 
ff_tests <- plot_cm %>% filter(model=='FairFace')
# ff_results_conf <- confusionMatrix(data=ff_tests$p_value_classification,reference=ff_tests$f1_classification)
df_tests <- plot_cm %>% filter(model=='DeepFace')
# df_results_conf <- confusionMatrix(data=df_tests$p_value_classification,reference=df_tests$f1_classification,)$table


```

In @fig-p-conf, we display confusion matrices of the TODO

```{r, warning=F}
#| label: fig-p-conf
#| fig-cap: Confusion matrices of TODO
#| fig-subcap: 
#|   - Matrix for FairFace
#|   - Matrix for DeepFace
#| layout-ncol: 2

# BJ
autoplot(conf_mat(ff_tests, p_value_classification, f1_classification), type = "heatmap") +
  scale_fill_distiller(palette = "YlGn", direction = 1) +
  labs(title="FairFace")

autoplot(conf_mat(df_tests, p_value_classification, f1_classification), type = "heatmap") +
  scale_fill_distiller(palette = "YlGn", direction = 1) +
  labs(title="DeepFace")
```


<!-- ### Statistical Power - TODO REMOVE -->
<!-- $$
\beta = P\bigg(\bigg|\frac{\sqrt{n}\cdot\hat{p}-p_a}{\sqrt{p_a(1-p_a)}}\bigg|\geq\frac{\sqrt{n}\cdot p_0-p_a}{\sqrt{p_0(1-p_0)}}\bigg)
$$ -->


<!-- Our selected level of significance is 99.7% (3-sigma). Type-II error is denoted by $\beta$ above, and Power will be $1-\beta$

With $p_0$ being our *assumed* population proportion (from the source dataset and what we used in our tests), $p_a$ being the *actual* population proportion (from one or more of the below methods), $n$ being the number of predicted members of a racial group (i.e. "Indian"),

-   For Gender - assume that sex at birth is a bernoulli trial, over time, the proportion for both genders should be 0.5

-   For age groups - assume that age has a true normal distribution. Each race may have different means and standard deviations for their distribution of age, but still adhere to a normal distribution. The "population" proportions may be a bit more challenging to calculate, but under this framework, we may be able to get there.

    -   May be able to get via bootstrapping the source dataset, average age by race - I think that's what we did in our last project?

    -   Could look at external data? May not have time to look through everything. -->

```{r PowerCalcs}
#can be removed.
# TypeII <- function(p0,pa,n){
#   z <- (sqrt(n)*(p0-pa))/(sqrt(p0*(1-p0)))
# }

```


