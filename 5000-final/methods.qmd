# Methods

```{r setup, include=FALSE}
#| include: false
library(tidyverse)
```

<!-- BJ !-->

<!-- CK !-->

@fairface

## The Big Picture

-   Is bias prevalent in facial recognition machine learning models?
-   Can one model be shown to have statistically significant less bias than the other?
-   Does one model outperform the other in a statistically significant manner, in all aspects?
-   Does one model outperform the other in a statistically significant manner, in certain aspects?
    -   This is where we can dive into "conventional" bias

- Are there disparate outcomes (i.e. lower chances of correct classification) for one racial group vs. another?

::: callout-note
## Thoughts on Bias

We need to be careful how we define and use bias. Statistical bias is essentially error, and we could be crossing our definitions between statistical bias and conventional bias.
:::

## Measuring Performance

::: callout-note
This performance section is important in choosing the correct models to ensure data integrity, however for the actual statistical tests, we'll focused on more common statistics like mean and proportion.

<!-- PC --> 
NOTE:  From Rubric - 
May need to relocate sub-sections from data back to here:

- Exploratory data analysis
- what modifications were necessary to make the dataset ready for analysis?

Separate of those two - may need explanations on the analyses we performed and **why we think they are appropriate.**  We have part of that regarding the F1 use to possibly identify errors, may need more on two-prop tests and the like

<!-- end PC-->

<!-- PC -->
These are PC's recommendations on how we can look at the additional values: 

For cases in which we reject the statistical null hypothesis, we plan to evaluate the below metrics for the same output category.  Should we reject a null hypothesis for, let's say, the proportion of females, given that they are asian, we will examine the accuracy and F-1 scores for the same category.  We will consider the following range of values:

* F1 Score < .9 vs F1 Score >= 0.9
* Power < .8 vs Power >= .8
* p-value < .003 vs pvalue >= .003

<!-- PC --> 
:::

There are four main measures of performance when evaluating a model:

-   **Accuracy**
-   **Precision**
-   **Recall**
-   **F1-Score**

Each of these performance measures has their own place in evaluating models, however, to begin to explain the differences between these models we should start with concepts of positive and negative outcomes.

-   **True Positive:** predicted positive, was actually positive (correct)
-   **False Positive:** predicted positive, was actually negative (incorrect)
-   **True Negative:** predicted negative, was actually negative (correct)
-   **False Negative:** predicted negative, was actually positive (incorrect)

These outcomes can be visualized on a confusion matrix. In the image below, green are correct predictions while red are incorrect predictions.

![confusion_matrix](images/confusion_matrix.png)

### Accuracy

**Accuracy** is the ratio of correct predictions to all predictions. In other words, the total of the green squares divided by the entire matrix. This is arguably the most common concept of measuring performance.

$Acccuracy = \frac{TP+TN}{TP + TN + FN}$

### Precision

**Precision** is the ratio of true positives to the total number of positives (true positive + true negative).
```{r}
#$Precision = \frac{TP}{TP+FP}$
```
### Recall

**Recall** is the ratio of true positives to the number of total correct predictions (true positive + false negative).

```{r}
##$Recall = \frac{TP}{TP+FN}$
```

### F1-Score

**F1-Score**\* is known as the harmonic mean between precision and recall. **Precision** and **Recall** are useful in their own rights, but the f1-Score is useful in the fact it's a balanced combination of both precision and recall.

F1-Score $= \frac{2 * Precision * Recall}{Precision + Recall}$

When considering the classification of a subject by protected classes of age, gender, and race, we believe that stronger penalties should be assigned in making an improper classification decision.  Due to F1 being the harmonic mean of precision and recall, incorrect classification will more directly impact the score of each model in its prediction of protected classes, and do so more strongly than an accuracy calculation.

(Citations -
https://www.statology.org/f1-score-vs-accuracy/
https://medium.com/analytics-vidhya/accuracy-vs-f1-score-6258237beca2
)

We calculate F1 score as a measure of performance of our selected machine learning models. This was not used in the calculation or results of the hypothesis tests, but will be used for when we draw conclusions of our tests based upon p-value and statistical power.  Namely, we do not plan to control for statistical power / Type-II error when running our 432 hypothesis tests, so statistical power may vary from test to test.  Using F1 scores to assess p-values in cases of low statistical power should assist us in identifying potential Type-II Errors.  We set a F1 score threshold of 0.9 to make this determination.

In lamens terms, we're leveraging F1 score as a mitigatory factor for our team to avoid stating that a model is biased when it may not be.

## Hypothesis Testing

Our data consists of three main sets, the source input data, the Fairface output data, and the Deepface output data.

We'll be creating our hypothesis tests by treating the source data as the basis for the original assumptions (our *null hypotheses*), and then using the output from Fairface and Deepface to test for statistically significant differences. Gaining a statistically significant result would allow us to reject our *null hypothesis* in favor of the *alternative hypothesis*. In other words, rejecting the original assumption means there is a statistically large enough difference between the source data and output data, and could indicate a bias in model.

We'll be testing across different subsets contained within the data, as listed below:

### Demographics

-   Age Group
-   Gender
-   Race

### Demographics' Subgroups

+ Age Group (9 groups)
  + 0-2
  + 3-9
  + 10-19
  + 20-29
  + 30-39
  + 40-49
  + 50-59
  + 60-69
  + 70-130
+ Gender (2 groups)
  + Female
  + Male
+ Race (5 groups)
  + Asian
  + Black
  + Indian
  + Other
  + White

### The General Proportion Tests

Our hypothesis tests will be testing different proportions within these subgroups between the source data and the output data.

The general format of our hypothesis tests will be:

$H_0: p = p_{\text{Source Data Subset}}$

$H_A: p \neq p_{\text{Source Data Subset}}$

With the following test statistic:

$\frac{\sqrt{n}(\hat{p} - p)}{\sqrt{p(1 - p)}}$

With the p-value being calculated by:

$P(|Z| > \hat{p} | H_0)$

$= P(|Z| > \frac{\sqrt{n}(\hat{p} - p)}{\sqrt{p(1 - p)}})$,

where

- $n$: output data subset size
- $\hat{p}$: output data subset proportion
- $p$: source data subset proportion

<!-- PC -->
We believe that using two-sample proportion testing is an appropriate means by which we can evaluate the outputs of the two facial recognition models in comparison to the source data.  In leveraging two-sample proportion tests, we can infer whether or not the proportions of age, gender, or race (or some combination thereof) from the UTKFace dataset (i.e. 1st sample) originate from the same population as the outputs from each facial recognition model (i.e. 2nd dataset).  

**What is our population? Pictures of people across the internet?  Or pictures of people?**

In theory, similar proportions of protected classes between the two datasets could suggest that the source data and predicted data originate from the same population (pictures of people), and would thus indicate an absence of bias against the protected class in question.  Vastly different proportions, however, could indicate that the source data and predicted data are from differing populations and indicate a bias against the protected classes in question.

Leveraging p-values and powers calculated on our samples for our protected classes of age, gender, and race, should enable us to provide a clear picture of any biases that may manifest from one or both models.  Leveraging F1 scores in cases of inconclusive results from p-value and power should assist us in identifying potential error.

<!-- end PC-->

### Notation

Before we list the specific tests, we should introduce some notation.

Let $R$ be race, then $R \in \{Asian, Black, Indian, Other, White\} = \{A, B, I, O, W\}$

Let $G$ be gender, then $G \in \{Female, Male\} = \{F, M\}$

Let $A$ be age, then $A \in \{[0,2], [3,9], [10,19], [20,29], [30,39], [40,49], [50,59], [60,69], [70,130]\} = \{1, 2, 3, 4, 5, 6, 7, 8, 9\}$

Let $D$ be the dataset, then $D \in \{Source, Fairface, Deepface\} = \{D_0, D_f, D_d\}$


### More Specific Proportion Tests

Using this notation, we can simplify our nomenclature for testing a certain proportion of an overall demographic.

For example, we can test if the proportion of *Female* in the Fairface output is statistically different than the proportion of *Female* from the source.

Hypothesis Test:

$H_0: p_F = p_{F|D_0}$

$H_A: p_F \neq p_{F|D_0}$

P-value Calculation:

$P(|Z| > \frac{\sqrt{n}(\hat{p} - p)}{\sqrt{p(1 - p)}})$,

where

- $p = p_{F|D_0}$: proportion of females from the source data
- $\hat{p} = p_{F|D_f}$: proportion of females from the fairface output
- $n = n_{F \cup M|D_f}$: number of data points in the gender subset form the fairface output


Additionally, we could test for different combinations of subsets within demographics. For instance, if we wanted to test for a statistically significant difference between the proportion of those who *Female*, given that they were *Black*, then we could write a hypothesis test like:

$H_0: p_{F|B} = p_{F|D_0 \cap B}$

$H_A: p_{F|B} \neq p_{F|D_0 \cap B}$

P-value Calculation:

$P(|Z| > \frac{\sqrt{n}(\hat{p} - p)}{\sqrt{p(1 - p)}})$,

where

- $p = p_{F|D_0 \cap B}$: proportion of females from the source data, given they were black
- $\hat{p} = p_{F|D_f \cap B}$: proportion of females from the fairface output, given they were black
- $n = n_{F \cup M|D_f \cap B}$: number of data points in the gender subset form the fairface output, given they were black.

These were two specific hypothesis tests, however, we'll be testing many combinations of these parameters and reporting back on any significant findings.

<!--PC-->

In the above, we've outlined our methods for examining a total of 432 hypothesis tests per recognition model on the totality of, and smaller samples of, our overall dataset.  We have elected to sub-divide our source and predicted samples by these protected classes to inspect and investigate whether or not there may be bias against groupings of protected classes.  

For instance, in the performance of our hypothesis tests, we may find an absence of bias when only examining proportions of gender between samples.  However, by examining a subset of our samples, such as subject gender given the subject's membership in a specific racial category, we may find biases providing more, or fewer, correct predictions of subject gender given their membership in a specific racial group.

This could help us answer questions and draw conclusions about such groups.  For example: 

"Model X demonstrates bias in favor of correctly predicting race, given the subject is young." - which also suggests a bias against correctly predicting the race of older subjects in a model.  Such a bias, if used in a decision making process, could result in age discrimination.

"Model Y demonstrates bias against predicting correct gender, given the subject image is Black, Asian, or Other." - which also suggests a bias for correctly predicting gender, given the subject is White or Indian.  Such a bias, if used in a decision making process, could result in racial discrimination.

Structuring our tests in this manner will enable us to quickly analyze and report on the results of our tests.

<!--end PC-->

::: callout-note
## From the report requirements

Also can be called "Analyses"

This section might contain several subsections as needed.

-   At least one subsection should describe the exploratory data analysis you did.

-   What modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)

-   Describe the analyses you did to answer the question of interest. **Explain why you believe these methods are appropriate.**

-   At least one subsection should describe the exploratory data analysis you did.

-   What modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)

-   Describe the analyses you did to answer the question of interest. **Explain why you believe these methods are appropriate.**

-   At least one subsection should describe the exploratory data analysis you did.

-   What modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)

-   Describe the analyses you did to answer the question of interest. **Explain why you believe these methods are appropriate.**

Some methods we learn in this class include distribution comparison, correlation analysis, and hypothesis testing. You are required to include hypothesis tests into the project, but feel free to use additional methods to tell a good story about the data.

:::