# Methods

```{r setup, include=FALSE}
#| include: false
library(tidyverse)
```

<!-- BJ !-->

<!-- CK !-->

@fairface

## The Big Picture

-   Is bias prevalent in facial recognition machine learning models?
-   Can one model be shown to have statistically significant less bias than the other?
-   Does one model outperform the other in a statistically significant manner, in all aspects?
-   Does one model outperform the other in a statistically significant manner, in certain aspects?
    -   This is where we can dive into "conventional" bias

- Are there disparate outcomes (i.e. lower chances of correct classification) for one racial group vs. another?

::: callout-note
## Thoughts on Bias

We need to be careful how we define and use bias. Statistical bias is essentially error, and we could be crossing our definitions between statistical bias and conventional bias.
:::

## Measuring Performance

::: callout-note
This performance section is important in choosing the correct models to ensure data integrity, however for the actual statistical tests, we'll focused on more common statistics like mean and proportion.

<!-- PC -->
These are my recommendations on how we can look at the additional values: 

For cases in which we reject the statistical null hypothesis, we plan to evaluate the below metrics for the same output category.  Should we reject a null hypothesis for, let's say, the proportion of females, given that they are asian, we will examine the accuracy and F-1 scores for the same category.  We will consider the following range of values:

* Accuracy < 70: poor/unacceptable performance

* 70 < Accuracy < 79: marginally acceptable performance

* 80 < Accuracy < 89: acceptable performance

* 90 < Accuracy < 99: excellent performance

* **Do we need anything else regarding precision or recall? and does this all need to be a table?**
<!-- PC --> 
:::

There are four main measures of performance when evaluating a model:

-   **Accuracy**
-   **Precision**
-   **Recall**
-   **F1-Score**

Each of these performance measures has their own place in evaluating models, however, to begin to explain the differences between these models we should start with concepts of positive and negative outcomes.

-   **True Positive:** predicted positive, was actually positive (correct)
-   **False Positive:** predicted positive, was actually negative (incorrect)
-   **True Negative:** predicted negative, was actually negative (correct)
-   **False Negative:** predicted negative, was actually positive (incorrect)

These outcomes can be visualized on a confusion matrix. In the image below, green are correct predictions while red are incorrect predictions.

![confusion_matrix](images/confusion_matrix.png)

### Accuracy

**Accuracy** is the ratio of correct predictions to all predictions. In other words, the total of the green squares divided by the entire matrix. This is arguably the most common concept of measuring performance.

$Acccuracy = \frac{TP+TN}{TP + TN + FN}$

### Precision

**Precision** is the ratio of true positives to the total number of positives (true positive + true negative).

$Precision = \frac{TP}{TP+FP}$

### Recall

**Recall** is the ratio of true positives to the number of total correct predictions (true positive + false negative).

$Recall = \frac{TP}{TP+FN}$

### F1-Score

**F1-Score**\* is known as the harmonic mean between precision and recall. **Precision** and **Recall** are useful in their own rights, but the f1-Score is useful in the fact it's a balanced combination of both precision and recall.

F1-Score $= \frac{2 * Precision * Recall}{Precision + Recall}$

## Hypothesis Testing

Our data consists of three main sets, the source input data, the Fairface output data, and the Deepface output data.

We'll be creating our hypothesis tests by treating the source data as the basis for the original assumptions (our *null hypotheses*), and then using the output from Fairface and Deepface to test for statistically significant differences. Gaining a statistically significant result would allow us to reject our *null hypothesis* in favor of the *alternative hypothesis*. In other words, rejecting the original assumption means there is a statistically large enough difference between the source data and output data, and could indicate a bias in model.

We'll be testing across different subsets contained within the data, as listed below:

### Demographics

-   Age Group
-   Gender
-   Race

### Demographics' Subgroups

+ Age Group (9 groups)
  + 0-2
  + 3-9
  + 10-19
  + 20-29
  + 30-39
  + 40-49
  + 50-59
  + 60-69
  + 70-130
+ Gender (2 groups)
  + Female
  + Male
+ Race (5 groups)
  + Asian
  + Black
  + Indian
  + Other
  + White

### The General Proportion Tests

Our hypothesis tests will be testing different proportions within these subgroups between the source data and the output data.

The general format of our hypothesis tests will be:

$H_0: p = p_{\text{Source Data Subset}}$

$H_A: p \neq p_{\text{Source Data Subset}}$

With the following test statistic:

$\frac{\sqrt{n}(\hat{p} - p)}{\sqrt{p(1 - p)}}$

With the p-value being calculated by:

$P(|Z| > \hat{p} | H_0)$

$= P(|Z| > \frac{\sqrt{n}(\hat{p} - p)}{\sqrt{p(1 - p)}})$,

where

- $n$: output data subset size
- $\hat{p}$: output data subset proportion
- $p$: source data subset proportion

<!--PC-->
**NOTE** - may be worthwhile to state that we evaluated all cases here, but reduction to specific cases of controlling race and evaluating solely on gender or solely on race is of the most value to us.  Performing tight filtering down to, for instance, the proportion of white people given they are 40-49 and female may be too restrictive.

Performing and analyzing calcuations of P(Age|Race) or P(Gender|Race) are of more interest, as I think our research questions are looking at disparate impact to the outcomes for each racial group.
<!-- END PC --> 
### Notation

Before we list the specific tests, we should introduce some notation.

Let $R$ be race, then $R \in \{Asian, Black, Indian, Other, White\} = \{A, B, I, O, W\}$

Let $G$ be gender, then $G \in \{Female, Male\} = \{F, M\}$

Let $A$ be age, then $A \in \{[0,2], [3,9], [10,19], [20,29], [30,39], [40,49], [50,59], [60,69], [70,130]\} = \{1, 2, 3, 4, 5, 6, 7, 8, 9\}$

Let $D$ be the dataset, then $D \in \{Source, Fairface, Deepface\} = \{D_0, D_f, D_d\}$


### More Specific Proportion Tests

Using this notation, we can simplify our nomenclature for testing a certain proportion of an overall demographic.

For example, we can test if the proportion of *Female* in the Fairface output is statistically different than the proportion of *Female* from the source.

Hypothesis Test:

$H_0: p_F = p_{F|D_0}$

$H_A: p_F \neq p_{F|D_0}$

P-value Calculation:

$P(|Z| > \frac{\sqrt{n}(\hat{p} - p)}{\sqrt{p(1 - p)}})$,

where

- $p = p_{F|D_0}$: proportion of females from the source data
- $\hat{p} = p_{F|D_f}$: proportion of females from the fairface output
- $n = n_{F \cup M|D_f}$: number of data points in the gender subset form the fairface output


Additionally, we could test for different combinations of subsets within demographics. For instance, if we wanted to test for a statistically significant difference between the proportion of those who *Female*, given that they were *Black*, then we could write a hypothesis test like:

$H_0: p_{F|B} = p_{F|D_0 \cap B}$

$H_A: p_{F|B} \neq p_{F|D_0 \cap B}$

P-value Calculation:

$P(|Z| > \frac{\sqrt{n}(\hat{p} - p)}{\sqrt{p(1 - p)}})$,

where

- $p = p_{F|D_0 \cap B}$: proportion of females from the source data, given they were black
- $\hat{p} = p_{F|D_f \cap B}$: proportion of females from the fairface output, given they were black
- $n = n_{F \cup M|D_f \cap B}$: number of data points in the gender subset form the fairface output, given they were black.

These were two specific hypothesis tests, however, we'll be testing many combinations of these parameters and reporting back on any significant findings.


::: callout-note
## From the report requirements

Also can be called "Analyses"

This section might contain several subsections as needed.

-   At least one subsection should describe the exploratory data analysis you did.

-   What modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)

-   Describe the analyses you did to answer the question of interest. **Explain why you believe these methods are appropriate.**

-   At least one subsection should describe the exploratory data analysis you did.

-   What modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)

-   Describe the analyses you did to answer the question of interest. **Explain why you believe these methods are appropriate.**

-   At least one subsection should describe the exploratory data analysis you did.

-   What modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)

-   Describe the analyses you did to answer the question of interest. **Explain why you believe these methods are appropriate.**

Some methods we learn in this class include distribution comparison, correlation analysis, and hypothesis testing. You are required to include hypothesis tests into the project, but feel free to use additional methods to tell a good story about the data.

:::