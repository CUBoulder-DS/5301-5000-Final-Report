{
  "hash": "5e75b0c75b8e5ff560afb8dcaa55b216",
  "result": {
    "markdown": "# Methods\n\n\n\n\n\n\n\n<!-- BJ !-->\n<!-- CK !-->\n\n## The Big Picture\n\n- Is bias prevalent in facial recognition machine learning models?\n- Can one model be shown to have statistically significant less bias than the other?\n- Does one model outperform the other in a statistically significant manner, in all aspects?\n- Does one model outperform the other in a statistically significant manner, in certain aspects?\n  - This is where we can dive into \"conventional\" bias\n\n::: callout-note\n## Thoughts on Bias\n\nWe need to be careful how we define and use bias. Statistical bias is essentially error, and we could be crossing our definitions between statistical bias and conventional bias.\n:::\n\n## Measuring Performance\n\nThere are four main measures of performance when evaluating a model:\n\n- **Accuracy**\n- **Precision**\n- **Recall**\n- **F1-Score**\n\nEach of these performance measures has their own place in evaluating models, however, to begin to explain the differences between these models we should start with concepts of positive and negative outcomes.\n\n- **True Positive:** predicted positive, was actually positive (correct)\n- **False Positive:** predicted positive, was actually negative (incorrect)\n- **True Negative:** predicted negative, was actually negative (correct)\n- **False Negative:** predicted negative, was actually positive (incorrect)\n\nThese outcomes can be visualized on a confusion matrix. In the image below, green are correct predictions while red are incorrect predictions.\n\n![confusion_matrix](images/confusion_matrix.png)\n\n### Accuracy\n\n**Accuracy** is the ratio of correct predictions to all predictions. In other words, the total of the green squares divided by the entire matrix. This is arguably the most common concept of measuring performance.\n\n$Acccuracy = \\frac{TP+TN}{TP + TN + FN}$\n\n### Precision\n\n**Precision** is the ratio of true positives to the total number of positives (true positive + true negative).\n\n$Precision = \\frac{TP}{TP+FP}$\n\n### Recall\n\n**Recall** is the ratio of true positives to the number of total correct predictions (true positive + false negative).\n\n$Recall = \\frac{TP}{TP+FN}$\n\n### F1-Score\n\n**F1-Score*** is known as the harmonic mean between precision and recall. **Precision** and **Recall** are useful in their own rights, but the f1-Score is useful in the fact it's a balanced combination of both precision and recall.\n\nF1-Score $= \\frac{2 * Precision * Recall}{Precision + Recall}$\n\n## Conditional Peformance\n\nJust as with the exploratory analysis on the data set prior to running either model, it's helpful to garner an understanding of what the results were using basic statistics and metrics. By calculating metrics in a cascading fashion, starting with the overall performance and then zeroing in on subgroups, we can get an idea if and where bias is worth investigating.\n\n### Performance - Overall\n\n- Confusion Matrix / Performance Measures for Fairface (Fairface Results)\n- Confusion Matrix / Performance Measures for Deepface (Deepface Results)\n\n### Performance - Main Demographics\n\n- Age Group\n  - Fairface Results\n  - Deepface Results\n- Gender\n  - Fairface Results\n  - Deepface Results\n- Race\n  - Fairface Results\n  - Deepface Results\n  \n### Performance - Demographics' Subgroups\n\n- Age Group (9 groups)\n  - Fairface Results\n  - Deepface Resultseepface Results\n- Gender (2 groups)\n  - Fairface Results\n  - Deepface Results\n- Race (5 groups)\n  - Fairface Results\n  - Deepface Results\n\n## Hypothesis Testing\n\nNow that we have an idea how each of these models performed in general and across localized subsets, let's see if our models truly have statistically significant results and if bias is present.\n\nOur tests will focus on proportions, and will discover if there is statistically significant differences between and within the models.\n\n### Proportion Test\n\nThe **Proportion Test** will be useful when measuring difference in overall performance and single categories between models.\n\nThe *hypothesis test* we'll be using is:\n\n- $H_0: \\pi_2 - \\pi_1 = 0$\n\n- $H_A: \\pi_2 - \\pi_1 \\neq 0$\n\nA look at the test statistic:\n\n$Z = \\frac{p_2 - p_1 - \\pi_0}{\\sqrt{p^*(1-p^*)(\\frac{1}{n_1}+\\frac{1}{n_2})}}$, where\n\nthe *pooled proportion*, $p^*$ is calculated as\n\n$p^* = \\frac{x_1 + x_2}{n_1 + n_2}$\n\nA look at the code:\n\n\n\n::: {.cell}\n\n:::\n\n\n\n- x: vector of successes and failures\n\n- n: vector of counts of trials (can be ignored if x is a matrix or a table)\n\n- alternative: we can change if we have a specific equality to test\n\n- conf.level: 0.95 is the default (not pertinent to specify)\n\nWe can also build **confidence intervals**. We have 95% confidence that the true difference between the props lies within the following:\n\n\n\n::: {.cell}\n\n:::\n\n\n\n### Chi-Squared Test\n\nThe **Chi-squared Test** will be useful when testing across k-levels of a categorical variable. For instance, this could be useful when testing proportion within a demographic for a singe model (i.e. accuracy across race in Fairface).\n\nThe *hypothesis test* we'll be using is:\n\n*Equivalent Proportions (test for uniformity):*\n\n- $H_0: \\pi_1 = \\pi_2 = â€¦ = \\pi_n = \\pi$\n\n- $H_A: H_0$ is incorrect (at least one of the proportions is not equivalent)\n\nA look at the test statistic:\n\n$\\chi^2 = \\sum_{i=1}^{k} \\frac{(O_i - E_i)^2}{E_i}$, where\n\n$O_i$ is the observed count, and\n\n$E_i$ is the expected count.\n\nA look at the code:\n\n\n\n::: {.cell}\n\n:::\n\n\n\nImagine using this test across the categories of race. A test which favored $H_A$ could give credence to bias.\n\nThe standard is using the **Chi-squared Test** in testing for uniformity, however we can test specific proportions for each category. Here's a preview in R:\n\n\n\n::: {.cell}\n\n:::\n\n\n\nA possible use case of the non-uniform **Chi-Squared Test** could be to test if the proportion of inaccuracies follows the proportion of each demographic subcategory. For instance, does inaccuracy across race follow the proportions of race in the sample?\n\n### Non Parametric Cohort\n\n::: callout-note\n## From the report requirements\n\nAlso can be called \"Analyses\"\n\nThis section might contain several subsections as needed.\n\n<<<<<<< HEAD\n- At least one subsection should describe the exploratory data analysis you did.\n- What modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)\n- Describe the analyses you did to answer the question of interest. **Explain why you believe these methods are appropriate.**\n||||||| feac3d2\n-   At least one subsection should describe the exploratory data analysis you did.\n\n-   What modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)\n\n-   Describe the analyses you did to answer the question of interest. **Explain why you believe these methods are appropriate.**\n=======\n-   At least one subsection should describe the exploratory data analysis you did.\n\n-   What modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)\n\n    <!--PC!-->\n    ## Standardizing output\n\n    The model outputs for both FairFace and DeepFace do not conform to the categories provided within the University of Tennessee - Knoxville (UTK) dataset.  We elected to take the outputs from each model and modify them based upon the categories specified in the UTK dataset, namely:\n\n    \"[race] is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern).\"\n\n    \"[gender] is either 0 (male) or 1 (female)\"\n\n    \"[age] is an integer from 0 to 116, indicating the age\"\n\n    ### From FairFace\n\n    **Race**: The FairFace classification model had two options - one for \"fair7\" and one for \"fair4.\"  The latter provided predictions of race in the following categories:  [White, Black, Asian, Indian].  Of key note, the model omitted \"Other\" categories as listed in the race category for the UTK dataset.  However, the \"fair7\" model provides predictions across [White, Black, Latino_Hispanic, East Asian, Southeast Asian, Indian, Middle Eastern].  We elected to use the the fair7 model, and to refactor the output categories to match those of the UTK dataset. Namely, we refactored instances of Middle Eastern and Latino_Hispanic as \"Other,\" and instances of \"East Asian\" and \"Southeast Asian\" as \"Asian\"\n\n    **Age**: FairFace only provides a predicted age range as opposed to a specific, single, predicted age as a string.  To enable comparison of actual values to the predicted values, we maintained this column as a categorical variable, and split it into a lower and upper bound of predicted age as an integer.  This split will allow us to determine whether or not the prediction correctly binned the age (i.e. $lowerBound \\leq actualAge \\leq upperBound$), and if not - how far outside of those bounds the actual age lay.\n\n    **Gender**: no change to outputs of \"Male\" and \"Female.\"\n\n    ### From DeepFace\n\n    **Race**: Racial categorical output from DeepFace includes the following categories []\n\n    **Age**: DeepFace provides a prediction of a single, specific, predicted age.  We elected to match the predicted age to be the same range as would be predicted by Fair Face. For example, if DeepFace predicts an age like \"19,\" we assign it the same matching category as it would have in FairFace - \"10-19.\" From there, we also split this category into an upper and lower bound.  In spite of the fact that DeepFace does not provide any bounds or ranges on its age prediction outputs, to have a similar and fair comparison of both models, we give it those same upper and lower bounds for equitable comparison.\n\n    **Gender**: DeepFace outputs are \"Man\" and \"Woman\", and we refactor those values to \"Male\" and \"Female\" respectively.\n\n    ## Evaluating Permutations of Inputs and Models for Equitable Evaluation\n    \n    Aside from the differences in the outputs of each model in terms of age, race, and gender, there are also substantial differences between FairFace and DeepFace in terms of their available settings when attempting to categorize an image in each of these categories.\n\n    The need for this permutation evaluation rose from some initial scripting and testing of these models on a small sample of images from another facial dataset (the Asian Face Age Dataset).  We immediately grew concerned with DeepFace's performance using default settings (namely, enforcing requirement to detect a face prior to categorization, and using OpenCV as the default detection backend).  Running these initial scripting tests, we encountered a failure rate in DeepFace of approximately 70% in identifying and categorizing an image of a face.\n\n    We performed further exploratory analysis on both models in light of these facts, and sought some specific permutations of settings to determine what settings may provide the most fair and equitable comparison of the models prior to proceeding to further analysis.\n\n    ### DeepFace Analysis Options\n\n    DeepFace has a robust degree of avaialble settings when performing facial categorization and recognition.  These include enforcing facial detection prior to classification of an image, as well as 8 different facial detection models to detect a face prior to categorization.  The default of these settings is OpenCV detection with detection enabled.  Other detection backends include ssd, dlib, mtcnn, retinaface, mediapipe, yolov8, yunet, and fastmtcnn.  \n        \n    In a Python 3.8 environment, attempting to run detections using dlib, retinaface, mediapipe, yolov8, and yunet failed to run, or failed to install the appropriate models directly from source during exeuction.  Repairing any challenges or issues with the core functionality of DeepFace and FairFace's code is outside the scope of our work, and as such, we have excluded any of these non-functioning models from our permutation evaluation.\n\n    ### FairFace Analysis Options\n\n    The default script from FairFace provided no options via its command line script to change settings.  It uses dlib/resnet34 models for facial detection and image pre-processing, and uses its own fair4 and fair7 models for categorization.  There are no other options or flags that can be set by a user when processing a batch of images. \n        \n    We converted the simple script to a class in Python without addressing any feature bugs or errors in the underlying code.  This change provided us some additional options when performing the analysis of an input image using FairFace - namely, the ability to analyze and categorize an image with or without facial detection, similar to the functionality of DeepFace.  FairFace remains limited in the fact that is only detection model backend is built in dlib, but this change gives us more options when considering what type of images to use and what settings to use on both models before generating our final dataset for analysis.\n\n    ### Specific Permutations\n\n    With the above options in mind, we designed the following permutations for evaluation on a subset of the UTK dataset:\n\n    **Detection**       **Detection Model**                                         **Image source**\n    Enabled         Fairface=Dlib; DeepFace=[opencv,mtcnn,fastmtcnn,ssd]    Pre-cropped, in-the-wild\n    Disabled        None                                                    Pre-cropped, in-the-wild\n\n    For 8 total comparisons across a subset of X sampled images from UTK.\n    <!--End PC!-->\n    ### Permutation sample results (LN & DV)\n\n\n\n    (enforcement of facial detection, detection backend model, and cropped images vs. faces in-the-wild)\n\n-   Describe the analyses you did to answer the question of interest. **Explain why you believe these methods are appropriate.**\n>>>>>>> d1ea1b4bc252c28fc569327e08036166d004f57a\n\nSome methods we learn in this class include distribution comparison, correlation analysis, and hypothesis testing. You are required to include hypothesis tests into the project, but feel free to use additional methods to tell a good story about the data.\n:::\n",
    "supporting": [
      "methods_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}