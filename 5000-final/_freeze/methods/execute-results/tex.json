{
  "hash": "42f2551f1e9101336afb4697d3ff647b",
  "result": {
    "markdown": "# Methods\n\n\n\n\n\n\n\n<!-- BJ !-->\n## The Big Picture\n\n- Is Bias Prevalant in facial recognition machine learning models?\n- Can one model be shown to have statistically significant less bias than the other?\n- Does one model outperform the other in a statistically significant manner, in all aspects?\n- Does one model outperform the other in a statistically significant manner, in certain aspects?\n  - This is where we can dive into \"conventional\" bias.\n  \n::: callout-note\n## Thoughts on Bias\n\nWe need to be careful how we define and use bias. Statistical bias is essentially error, and we could be crossing our definitions between statistical bias and conventional bias.\n:::\n\n\n## Measuring Performance\n\nThere are four main measures of performance when evaluating a model:\n- **Accuracy**\n- **Precision**\n- **Recall**\n- **F1-Score**\n\nEach of these performance measures has their own place in evaluating models, however, to begin to explain the differences between these models we should start with concepts of positive and negative outcomes.\n\n- **True Positive:** predicted positive, was actually positive (correct)\n- **False Positive:** predicted positive, was actually negative (incorrect)\n- **True Negative:** predicted negative, was actually negative (correct)\n- **False Negative:** predicted negative, was actually positive (incorrect)\n\nThese outcomes can be visualized on a confusion matrix. In the image below, green are correct predictions while red are incorrect predictions.\n\n![confusion_matrix](images/confusion_matrix.png)\n\n\n### Accuracy\n\n**Accuracy** is the ratio of correct predictions to all predictions. In other words, the total of the green squares divided by the entire matrix. This is arguably the most common concept of measuring performance.\n\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$\n\n\n### Precision\n\n**Precision** is the ratio of true positives to the total number of positives (true positive + true negative).\n\n$Precision = \\frac{TP}{TP+FP}$\n\n\n### Recall\n\n**Recall** is the ratio of true positives to the number of total correct predictions (true positive + false negative).\n\n$Recall = \\frac{TP}{TP+FN}$\n\n\n### F1-Score\n\n**F1-Score** is known as the harmonic mean between precision and recall. **Precision** and **Recall** are useful in their own rights, but the f1-Score is useful in the fact itâ€™s a balanced combination of both precision and recall.\n\n\nF1-Score $= \\frac{2 * Precision * Recall}{Precision + Recall}$\n\n\n## Conditional Peformance\n\nJust as with the exploratory analysis on the data set prior to running either model, it's helpful to garner an understanding of what the results were using basic statistics and metrics. By calculating metrics in a cascading fashion, starting with the overall performance and then zeroing in on subgroups, we can get an idea if and where bias is worth investigating.\n\n\n### Performance - Overall\n\n- Confusion Matrix / Performance Measures for Fairface (Fairface Results)\n- Confusion Matrix / Performance Measures for Deepface (Deepface Results)\n\n\n### Performance - Main Demographics\n\n- Age Group\n  - Fairface Results\n  - Deepface Results\n- Gender\n  - Fairface Results\n  - Deepface Results\n- Race\n  - Fairface Results\n  - Deepface Results\n  \n  \n### Performance - Demographics' Subgroups\n\n- Age Group (9 groups)\n  - Fairface Results\n  - Deepface Results\n- Gender (2 groups)\n  - Fairface Results\n  - Deepface Results\n- Race (5 groups)\n  - Fairface Results\n  - Deepface Results\n\n\n## Hypothesis Testing\n\nNow that we have an idea how each of these models performed in general and across localized subsets, let's see if our models truly have statistically significant results and if bias is present.\n\nOur tests will focus on proportions, and will discover if there is statistically significant differences between and within the models.\n\n\n### Proportion Test\n\nThe **Proportion Test** will be useful when measuring difference in overall performance and single categories between models.\n\nThe *hypothesis test* we'll be using is:\n\n- $H_0: \\pi_2 - \\pi_1 = 0$\n- $H_A: \\pi_2 - \\pi_1 \\neq 0$\n\n\nA look at the test statistic:\n$Z = \\frac{p_2 - p_1 - \\pi_0}{\\sqrt{p^*(1-p^*)(\\frac{1}{n_1}+\\frac{1}{n_2})}}$, where\n\nthe *pooled proportion*, $p^*$ is calculated as\n\n$p^* = \\frac{x_1 + x_2}{n_1 + n_2}$\n\n\nA look at the code:\n\n\n::: {.cell}\n\n:::\n\n\n- x: vector of successes and failures\n- n: vector of counts of trials (can be ignored if x is a matrix or a table)\n- alternative: we can change if we have a specific equality to test\n- conf.level: 0.95 is the default (not pertinent to specify)\n\nWe can also build **confidence intervals**. We have 95% confidence that the true difference between the props lies within the following: \n\n\n::: {.cell}\n\n:::\n\n\n\n### Chi-Squared Test\n\nThe **Chi-squared Test** will be useful when testing across k-levels of a categorical variable. For instance, this could be useful when testing proportion within a demographic for a singe model (i.e. accuracy across race in Fairface).\n\n\nThe *hypothesis test* we'll be using is:\n\n*Equivalent Proportions (test for uniformity):*\n- $H_0: \\pi_1 = \\pi_2 = ... = \\pi_n = \\pi$\n- $H_A: H_0$ is incorrect (at least one of the proportions is not equivalent)\n\n\nA look at the test statistic:\n$\\chi^2 = \\sum_{i=1}^{k} \\frac{(O_i - E_i)^2}{E_i}$, where\n\n$O_i$ is the observed count, and\n\n$E_i$ is the expected count.\n\n\nA look at the code:\n\n\n::: {.cell}\n\n:::\n\n\n\nImagine using this test across the categories of race. A test which favored $H_A$ could give credence to bias.\n\nThe standard is using the **Chi-squared Test** in testing for uniformity, however we can test specific proportions for each category. Here's a preview in R:\n\n\n\n::: {.cell}\n\n:::\n\n\n\nA possible use case of the non-uniform **Chi-squared Test** could be to test if the proportion of inaccuracies follows the proportion of each demographic subcategory. For instance, does inaccuracy across race follow the proportions of race in the sample?\n\n\n### Non Parametric Cohort\n\n\n\n::: callout-note\n## From the report requirements\n\nAlso can be called \"Analyses\"\n\nThis section might contain several subsections as needed.\n\n-   At least one subsection should describe the exploratory data analysis you did.\n\n-   What modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)\n\n-   Describe the analyses you did to answer the question of interest. **Explain why you believe these methods are appropriate.**\n\nSome methods we learn in this class include distribution comparison, correlation analysis, and hypothesis testing. You are required to include hypothesis tests into the project, but feel free to use additional methods to tell a good story about the data.\n:::\n\n",
    "supporting": [
      "methods_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}