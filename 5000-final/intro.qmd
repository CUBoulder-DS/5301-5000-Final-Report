# Introduction

```{r setup, include=FALSE}
#| include: false

library(tidyverse)
```



::: callout-note
## From the report requirements

This section introduces your problem to a **non-expert** audience, describes the context and history of the problem.

For example, if your overall project topic is on Diabetes Prevention and Prediction, then you would use the Introduction to introduce what diabetes is, who it affects, why prevention is important, history on diabetes prevention, etc.

Some questions that you could answer in the introduction:

-   What is the "research question"? why is it interesting or worth answering?

-   What is the relevant background information for readers to understand your project? Assume that your audience\
    is not an expert in the application field.

-   Is there any prior research on your topic that might be helpful for the audience?

The goal of the introduction it to capture the audience's interest in your paper. An introduction that starts with "Diabetes kills over 87 thousand people each year and in many cases may be preventable" is more engaging than "This paper is about diabetes prevention".

The introduction should be 2-4 paragraphs long.
:::

The issue of algorithmic bias, especially concerning sensitive and personal data, is an ongoing problem in today’s use of Artificial Intelligence (AI). Facial recognition is one field that is struggling with mitigating and minimizing the issue. According to a report by the National Institute of Standards and Technology, the rates of false positives, or misidentification, of African and East Asian faces were 10 to 100 times higher than those for White or European faces [citation here]. Numerous studies have found that many facial recognition algorithms, having been based and created in white-dominated spaces, often lack accuracy with darker faces, especially compared to their identification of white faces. This issue has caused numerous problems throughout the development of facial recognition. For instance, a Georgetown study found that African Americans were significantly misidentified in law enforcement databases, due to being overrepresented in mugshots [citation needed]. That sort of misinterpretation could lead to unlawful arrests, accusations, or sentencings. A facial recognition algorithm has two main areas where these sorts of biases occur: the actual coding/iteration, and the data used to train it. The databases used to teach an algorithm how to make decisions and identify faces matter, from the balance of different races, genders, and ages, to how well those databases use facial markers to identify anything. As facial recognition becomes more widespread, this becomes a key question of data ethics and misuse. 
Thus, it is necessary to examine existing algorithms for their accuracy in identifying faces properly. Two easily accessible algorithms that claim to do just that are FairFace, created by UCLA researchers [insert names here], and DeepFace, created by a team of researchers at Facebook. Both claim to accurately identify the race, gender, and age of any given photo. FairFace claims to have reduced bias compared to other common facial recognition algorithms. It was trained on a balanced dataset that featured equal numbers of different races, including Middle Eastern faces. The creators point out in their work that the majority of other training data is overwhelmingly white and male, lending those algorithms a bias [citation here]. The DeepFace algorithm was developed by a team at Facebook, now Meta, and also aims to be an accessible and accurate open-source facial recognition system. The creators have shown an accuracy rate of up to 97% on at least one dataset, but replication is key when it comes to testing code [citation needed]. It is this paper’s goal to test the accuracy of those claims and compare these algorithm’s ability to guess the race, gender, and age of a given dataset, both with and without controlling for the race of the given faces. Both will be tested against the UTKFace dataset, which consists of over 20,000 labeled faces that can be used for purposes such as this [citation needed].
Our testing spans the age group, gender, and race of the UTKFace dataset across the results for both the FairFace and DeepFace models. The null hypothesis is that the identified proportions p, for the FairFace and DeepFace outputs of a given demographic, will equal the proportion of that demographic present in the testing data. The alternative hypothesis is that the generated results will not equal the UTK proportions.

Ethics Bias Papers:

https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html?auth=login-email&login=email

https://www.perpetuallineup.org/

https://www.nist.gov/news-events/news/2019/12/nist-study-evaluates-effects-race-age-sex-face-recognition-software
