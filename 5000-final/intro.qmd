# Introduction

The issue of algorithmic bias, especially concerning sensitive and personal data, is an ongoing problem in today's use of Artificial Intelligence (AI). Facial recognition is one field that is struggling with mitigating and minimizing the issue. According to a report by the National Institute of Standards and Technology, the rates of false positives, or misidentification, of African and East Asian faces were 10 to 100 times higher than those for White or European faces [@nist]. Numerous studies have found that many facial recognition algorithms, having been based and created in white-dominated spaces, often lack accuracy with darker faces, especially compared to their identification of white faces. This issue has caused numerous problems throughout the development of facial recognition. For instance, a Georgetown study found that African Americans were significantly misidentified in law enforcement databases, due to being overrepresented in mugshots [@perplineup]. That sort of misinterpretation could lead to unlawful arrests, accusations, or sentencings. A facial recognition algorithm has two main areas where these sorts of biases occur: the actual coding/iteration, and the data used to train it. The databases used to teach an algorithm how to make decisions and identify faces matter, from the balance of different races, genders, and ages, to how well those databases use facial markers to identify anything. As facial recognition becomes more widespread, this becomes a key question of data ethics and misuse [@nytimes].

Thus, it is necessary to examine existing algorithms for their accuracy in identifying faces properly. Two easily accessible algorithms that claim to do just that are FairFace, created by UCLA researchers [@fairface], and DeepFace [@deepface], created by a team of researchers at Facebook. Both claim to accurately identify the race, gender, and age of any given photo. FairFace claims to have reduced bias compared to other common facial recognition algorithms. It was trained on a balanced dataset that featured equal numbers of different races, including Middle Eastern faces. The creators point out in their work that the majority of other training data is overwhelmingly white and male, lending those algorithms a bias [@fairface]. The DeepFace algorithm was developed by a team at Facebook, now Meta, and also aims to be an accessible and accurate open-source facial recognition system. The creators have shown an accuracy rate of up to 97% on at least one dataset, but replication is key when it comes to testing code [@deepface].

It is this paper's goal to test the accuracy of those claims and compare these algorithm's ability to guess the race, gender, and age of a given dataset, both with and without controlling for the race of the given faces. Both will be tested against the UTKFace dataset, which consists of over 20,000 labeled faces that can be used for purposes such as this [@utkdataset]. We will examine the bias present in either tested model using hypothesis testing and by inspecting performance metrics such as F1 score and accuracy, and compare this bias against the baseline (aka, the bias inherently present in the UTK face dataset).
