# Introduction

The issue of algorithmic bias, especially concerning sensitive and personal data, is an ongoing problem in today's use of Artificial Intelligence (AI). Facial recognition is one field that is struggling with mitigating and minimizing the issue. According to a report by the National Institute of Standards and Technology, the rates of false positives, or misidentification, of African and East Asian faces were 10 to 100 times higher than those for White or European faces [@nist]. Numerous studies have found that many facial recognition algorithms, having been based and created in white-dominated spaces, often lack accuracy with darker faces, especially compared to their identification of white faces. This issue has caused numerous problems throughout the development of facial recognition. For instance, a Georgetown study found that African Americans were significantly misidentified in law enforcement databases, due to being overrepresented in mugshots [@perplineup]. That sort of misinterpretation could lead to unlawful arrests, accusations, or sentencings. A facial recognition algorithm has two main areas where these sorts of biases occur: the actual coding/iteration, and the data used to train it. The databases used to teach an algorithm how to make decisions and identify faces matter, from the balance of different races, genders, and ages, to how well those databases use facial markers to identify anything. As facial recognition becomes more widespread, this becomes a key question of data ethics and misuse [@nytimes].

Thus, it is necessary to examine existing algorithms for their accuracy in identifying faces properly. Two easily accessible algorithms that claim to do just that are FairFace, created by UCLA researchers [@fairface], and DeepFace [@deepface], created by a team of researchers at Facebook. Both claim to accurately identify the race, gender, and age of any given photo. FairFace claims to have reduced bias compared to other common facial recognition algorithms. FairFace was trained on a balanced dataset, eqully stratified across race, including Middle Eastern Faces.<!--It was trained on a balanced dataset that featured equal numbers of different races, including Middle Eastern faces.--> The creators point out in their work that the majority of training datasets overwhelmingly represent white and male subjects, lending to algorithmic biases in any models leveraging such data for training<!--the majority of other training data is overwhelmingly white and male, lending those algorithms a bias--> [@fairface]. The DeepFace algorithm was developed by a team at Facebook, now Meta, and also aims to be an accessible and accurate open-source facial recognition system. In their paper on research and development of DeepFace, the creators claim 97% accuracy on gender prediction, but only 68% accuracy on race and ethnicity.  There is a more complex discussion of age prediction, and the creators further state that a previous study produced more accurate results when compared to the current model.  Furthermore, the current model was claimed to be less accurate than human-provided predictions<!--The creators have shown an accuracy rate of up to 97% on at least one dataset, but replication is key when it comes to testing code--> [@deepface].

<!--CK: After reviewing the rubric, moving the research questions from the methods section to the introduction section. The questions have also been edited for clarity.-->
**Research Questions**

- Are biases prevalent in facial recognition machine learning models?
- Can we find biases using proportionality testing and by more traditional measurements?
- How does proportionality testing compare to more traditional measurements?
- How do the results change when from an overall perspective versus specific subsets within the data?

<!--It is this paper's goal to test the accuracy of those claims and compare these algorithm's ability to guess the race, gender, and age of a given dataset, both with and without controlling for the race of the given faces-->
Our goal in this research is to test the strength of the models' claims and compare the algorithms' ability to predict age, gender, and race against a source dataset. Both will be tested against the UTKFace dataset, which consists of over 24,000 labeled faces that can be used for research purposes [@utkdataset]. We will identify potential biases in the modelsl using two-sample proportion hypothesis testing, and by inspect specific instances of such bias using performance metrics such as F1 score and accuracy. 

<!--and compare this bias against the baseline (aka, the bias inherently present in the UTK face dataset).-->
