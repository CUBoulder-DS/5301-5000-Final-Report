[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bias in Facial Classification ML Models",
    "section": "",
    "text": "Abstract\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nFeatures of Quarto:"
  },
  {
    "objectID": "index.html#how-we-should-write-this-report",
    "href": "index.html#how-we-should-write-this-report",
    "title": "Bias in Facial Classification ML Models",
    "section": "How we should write this report",
    "text": "How we should write this report\n\nSee Karkkainen and Joo (2021) , that is an example on how to cite a bibliography.\nSections/title headings are automatically numbered.\nAny changes you make, make sure to make a comment of your initials at the top of your work (INCLUDING written text) like so:\n\n&lt;!-- BJ !--&gt;\nBlah blah etc ....\n\nOR\n#BJ\nr_var &lt;- ...\n\nMake sure to add a unique name to all code cells, and to also enable the following (the quarto way) (In order for a figure to be cross-referenceable, its label must start with the fig- prefix):\n\n\n\n\n\n\nFigure 1: A caption for generated figure\n\n\n\n\n\nYou can then refer to figures like this @fig-sec1-unique-name Figure 1\nFormat tables doing the following Link here\nDo all your r work initially in your own custom .rmd file in this directory, so that it can be copy-pasted over later into the appropriate section (written descriptions/words can go straight into the .qmd files though). For example, Bhav’s work is in 5000-final/BJ_work.rmd.\n\n\n\n\n\n\n\nFrom the report requirements\n\n\n\nA 3-5 summary of the paper. It should address the research question, the methods, and the conclusions of your analysis.\n“A good recipe for an abstract is: first sentence: specify the general area of the paper and encourage the reader; second sentence: specify the dataset and methods at a general level; third sentence: specify the headline result; and a fourth sentence about implications.”\n\n\n\n\n\n\nKarkkainen, Kimmo, and Jungseock Joo. 2021. “FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation.” In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 1548–58."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "From the report requirements\n\n\n\nThis section introduces your problem to a non-expert audience, describes the context and history of the problem.\nFor example, if your overall project topic is on Diabetes Prevention and Prediction, then you would use the Introduction to introduce what diabetes is, who it affects, why prevention is important, history on diabetes prevention, etc.\nSome questions that you could answer in the introduction:\n\nWhat is the “research question”? why is it interesting or worth answering?\nWhat is the relevant background information for readers to understand your project? Assume that your audience\nis not an expert in the application field.\nIs there any prior research on your topic that might be helpful for the audience?\n\nThe goal of the introduction it to capture the audience’s interest in your paper. An introduction that starts with “Diabetes kills over 87 thousand people each year and in many cases may be preventable” is more engaging than “This paper is about diabetes prevention”.\nThe introduction should be 2-4 paragraphs long."
  },
  {
    "objectID": "data.html#exploration-of-source-data",
    "href": "data.html#exploration-of-source-data",
    "title": "2  Data",
    "section": "2.1 Exploration of Source Data",
    "text": "2.1 Exploration of Source Data\n\n\n\n\n\n\n\n(a) Age=6, Gender=F, Race=Indian\n\n\n\n\n\n\n\n(b) Age=38, Gender=M, Race=White\n\n\n\n\n\n\n\n(c) Age=80, Gender=M, Race=Asian\n\n\n\n\nFigure 2.1: Example face images from the UTK dataset (“UTKFace” 2021) with their associated given labels.\n\n\n\n\n\n\nFigure 2.2: An interactive figure showcasing the distributions of various data factors in the image dataset, and showcasing the underlying data.\n\n\n\n\n\n\n\n\nFrom the report requirements\n\n\n\nThis section should describe the data you’ll be using. Answer at least all of the following questions:\n\nHow was the data collected?\nWhat are the sources and influences of bias in the data?\nWhat are the important features (=columns) that you are using in your analysis? What do they mean?\n\nFeel free to add anything else that you think is necessary for understanding the paper and the context of the problem."
  },
  {
    "objectID": "data.html#more-information-on-data",
    "href": "data.html#more-information-on-data",
    "title": "2  Data",
    "section": "2.2 More information on data",
    "text": "2.2 More information on data\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\n\n\nfilenames\npurpose\nrecommendations\n\n\n\n\nCroped_ff_np.csv\nPermutation evaluation (older version) for fairface, no preprocessing on cropped images. Updated this file to look at the same files as the uncropped dataset.\nRemove from github data folder.\n\n\nMasterDataFrame.csv\nFinal master data file containing all input and output files\nKeep as-is with no changes\n\n\ncrop_df_np.csv\nPermutation evaluation for DeepFace, cropped images, no pre-processing\nRetain; rename to PERM_DF_c_np.csv\n\n\ncrop_df_p_mtcnn.csv\nPermutation evaluation for DeepFace, cropped images, preprocessed with MTCNN backend.\nRetain; rename to PERM_DF_c_p_mtcnn.csv\n\n\ncrop_df_p_opencv.csv\nPermutation evaluation for DeepFace, cropped images, preprocessed with OpenCV backend.\nRetain; rename to PERM_DF_c_p_opencv.csv\n\n\ncropped_UTK.csv\nPermutation evaluation (older version), list of cropped files to perform evaluation.\nRemove from github data folder\n\n\ncropped_UTK_dataset.csv\nPermutation evaluation (newest version), list of cropped files to perform evaluation.\nRetain with no changes\n\n\ncropped_ff_p.csv\nPermutation evaluation (older version), used older version of cropped images dataset.\nRemove from github data folder.\n\n\njoined_permutations.csv\nPermutation evaluation (newest version), joined all permutation outputs from DeepFace and FairFace to a single file\nRetain with no changes\n\n\nnew_ff_c_np.csv\nPermutation evaluation (newest version), FairFaice outputs for cropped images with no preprocessing\nRetain; rename to PERM_FF_c_np.csv\n\n\nnew_ff_c_p.csv\nPermutation evaluation (newest version), FairFaice outputs for cropped images with dlib preprocessing\nRetain; rename to PERM_FF_c_p.csv\n\n\nnew_ff_uc_np.csv\nPermutation evaluation (newest version), FairFaice outputs for uncropped images with no preprocessing\nRetain; rename to PERM_FF_uc_np.csv\n\n\nnew_ff_uc_p.csv\nPermutation evaluation (newest version), FairFaice outputs for uncropped images with dlib preprocessing.\nRetain; rename to PERM_FF_uc_p.csv\n\n\nnon_normalized_DeepFace_uncropped_DF_all.csv\nFinal dataset of DeepFace Outputs (non-normalized)\nRetain; rename to Master_DF_non_normalized.csv\n\n\nnon_normalized_FairFace_uncropped_FF_all.csv\nFinal dataset of FairFace Outputs (non-normalized)\nRetain; rename to Master_FF_non_normalized.csv\n\n\nuncropped_DF_all.csv\nFinal normalized output for DeepFace - used to build MasterDataFrame.csv\nRetain with no changes\n\n\nuncropped_FF_all.csv\nFinal normalized output for FairFace - used to build MasterDataFrame.csv\nRetain with no changes\n\n\nuncropped_UTK.csv\nPermutation evaluation (older version) - source data file for iteration script\nRemove from github data folder.\n\n\nuncropped_UTK_dataset.csv\nPermutation evaluation (newest version) - source data file for uncropped images in iteration script\nRetain with no changes\n\n\nuncropped_df_np.csv\nPermutation evaluation (newest version) - DeepFace uncropped images with no preprocessing\nRetain; rename to PERM_DF_uc_np.csv\n\n\nuncropped_df_p_mtcnn.csv\nPermutation Evaluation (newest version) - DeepFace uncropped images with mtcnn preprocessing\nRetain; rename to PERM_DF_uc_p_mtcnn.csv\n\n\nuncropped_df_p_opencv.csv\nPermutation Evaluation (newest version) - DeepFace uncropped images with opencv preprocessing\nRetain; rename to PERM_DF_uc_p_opencv.csv\n\n\nuncropped_ff_np.csv\nPermutation Evaluation (older version) - FairFace uncropped images with no preprocessing\nRemove from github data folder.\n\n\nuncropped_ff_p.csv\nPermutation Evaluation (older version) - FairFace uncropped images with dlib preprocessing.\nRemove from github data folder."
  },
  {
    "objectID": "data.html#data-selection-utk-dataset---ln-dv",
    "href": "data.html#data-selection-utk-dataset---ln-dv",
    "title": "2  Data",
    "section": "2.3 Data Selection (UTK Dataset) - LN DV",
    "text": "2.3 Data Selection (UTK Dataset) - LN DV\nMotivation - has there been progress against bias?\nReference Articles from Ethics Class - Joy B’s work, etc. Lots of disparity back in 2018, how much of that still exists with free models?"
  },
  {
    "objectID": "data.html#selected-models-ln-dv",
    "href": "data.html#selected-models-ln-dv",
    "title": "2  Data",
    "section": "2.4 Selected Models (LN DV)",
    "text": "2.4 Selected Models (LN DV)\nLorem ipsum\n\n2.4.1 FairFace\nMotivation as to how / why we came across this\n\n\n2.4.2 DeepFace\nMotivation as to how / why we came across this\n\n\n2.4.3 Permutation Analysis Information\n\n\n\n\n“UTKFace.” 2021. UTKFace. https://susanqq.github.io/UTKFace."
  },
  {
    "objectID": "methods.html#the-big-picture",
    "href": "methods.html#the-big-picture",
    "title": "3  Methods",
    "section": "3.1 The Big Picture",
    "text": "3.1 The Big Picture\n\nIs bias prevalent in facial recognition machine learning models?\nCan one model be shown to have statistically significant less bias than the other?\nDoes one model outperform the other in a statistically significant manner, in all aspects?\nDoes one model outperform the other in a statistically significant manner, in certain aspects?\n\nThis is where we can dive into “conventional” bias\n\n\n\n\n\n\n\n\nThoughts on Bias\n\n\n\nWe need to be careful how we define and use bias. Statistical bias is essentially error, and we could be crossing our definitions between statistical bias and conventional bias."
  },
  {
    "objectID": "methods.html#measuring-performance",
    "href": "methods.html#measuring-performance",
    "title": "3  Methods",
    "section": "3.2 Measuring Performance",
    "text": "3.2 Measuring Performance\n\n\n\n\n\n\nNote\n\n\n\nThis performance section is important in choosing the correct models to ensure data integrity, however for the actual statistical tests, we’ll focused on more common statistics like mean and proportion.\n\n\nThere are four main measures of performance when evaluating a model:\n\nAccuracy\nPrecision\nRecall\nF1-Score\n\nEach of these performance measures has their own place in evaluating models, however, to begin to explain the differences between these models we should start with concepts of positive and negative outcomes.\n\nTrue Positive: predicted positive, was actually positive (correct)\nFalse Positive: predicted positive, was actually negative (incorrect)\nTrue Negative: predicted negative, was actually negative (correct)\nFalse Negative: predicted negative, was actually positive (incorrect)\n\nThese outcomes can be visualized on a confusion matrix. In the image below, green are correct predictions while red are incorrect predictions.\n\n\n\nconfusion_matrix\n\n\n\n3.2.1 Accuracy\nAccuracy is the ratio of correct predictions to all predictions. In other words, the total of the green squares divided by the entire matrix. This is arguably the most common concept of measuring performance.\n\\(Acccuracy = \\frac{TP+TN}{TP + TN + FN}\\)\n\n\n3.2.2 Precision\nPrecision is the ratio of true positives to the total number of positives (true positive + true negative).\n\\(Precision = \\frac{TP}{TP+FP}\\)\n\n\n3.2.3 Recall\nRecall is the ratio of true positives to the number of total correct predictions (true positive + false negative).\n\\(Recall = \\frac{TP}{TP+FN}\\)\n\n\n3.2.4 F1-Score\nF1-Score* is known as the harmonic mean between precision and recall. Precision and Recall are useful in their own rights, but the f1-Score is useful in the fact it’s a balanced combination of both precision and recall.\nF1-Score \\(= \\frac{2 * Precision * Recall}{Precision + Recall}\\)"
  },
  {
    "objectID": "methods.html#hypothesis-testing",
    "href": "methods.html#hypothesis-testing",
    "title": "3  Methods",
    "section": "3.3 Hypothesis Testing",
    "text": "3.3 Hypothesis Testing\nOur data consists of three main sets, the source input data, the Fairface output data, and the Deepface output data.\nWe’ll be creating our hypothesis tests by treating the source data as the basis for the original assumptions (our null hypotheses), and then using the output from Fairface and Deepface to test for statistically significant differences. Gaining a statistically significant result would allow us to reject our null hypothesis in favor of the alternative hypothesis. In other words, rejecting the original assumption means there is a statistically large enough difference between the source data and output data, and could indicate a bias in model.\nWe’ll be testing across different subsets contained within the data, as listed below:\n\n3.3.1 Demographics\n\nAge Group\nGender\nRace\n\n\n\n3.3.2 Demographics’ Subgroups\n\nAge Group (9 groups)\n\n0-2\n3-9\n10-19\n20-29\n30-39\n40-49\n50-59\n60-69\n70-130\n\nGender (2 groups)\n\nFemale\nMale\n\nRace (5 groups)\n\nAsian\nBlack\nIndian\nOther\nWhite\n\n\n\n\n3.3.3 The General Proportion Tests\nOur hypothesis tests will be testing different proportions within these subgroups between the source data and the output data.\nThe general format of our hypothesis tests will be:\n\\(H_0: p = p_{\\text{Source Data Subset}}\\)\n\\(H_A: p \\neq p_{\\text{Source Data Subset}}\\)\nWith the following test statistic:\n\\(\\frac{\\sqrt{n}(\\hat{p} - p)}{\\sqrt{p(1 - p)}}\\)\nWith the p-value being calculated by:\n\\(P(|Z| &gt; \\hat{p} | H_0)\\)\n\\(= P(|Z| &gt; \\frac{\\sqrt{n}(\\hat{p} - p)}{\\sqrt{p(1 - p)}})\\),\nwhere\n\n\\(n\\): output data subset size\n\\(\\hat{p}\\): output data subset proportion\n\\(p\\): source data subset proportion\n\n\n\n3.3.4 Notation\nBefore we list the specific tests, we should introduce some notation.\nLet \\(R\\) be race, then \\(R \\in \\{Asian, Black, Indian, Other, White\\} = \\{A, B, I, O, W\\}\\)\nLet \\(G\\) be gender, then \\(G \\in \\{Female, Male\\} = \\{F, M\\}\\)\nLet \\(A\\) be age, then \\(A \\in \\{[0,2], [3,9], [10,19], [20,29], [30,39], [40,49], [50,59], [60,69], [70,130]\\} = \\{1, 2, 3, 4, 5, 6, 7, 8, 9\\}\\)\nLet \\(D\\) be the dataset, then \\(D \\in \\{Source, Fairface, Deepface\\} = \\{D_0, D_f, D_d\\}\\)\n\n\n3.3.5 More Specific Proportion Tests\nUsing this notation, we can simplify our nomenclature for testing a certain proportion of an overall demographic.\nFor example, we can test if the proportion of Female in the Fairface output is statistically different than the proportion of Female from the source.\nHypothesis Test:\n\\(H_0: p_F = p_{F|D_0}\\)\n\\(H_A: p_F \\neq p_{F|D_0}\\)\nP-value Calculation:\n\\(P(|Z| &gt; \\frac{\\sqrt{n}(\\hat{p} - p)}{\\sqrt{p(1 - p)}})\\),\nwhere\n\n\\(p = p_{F|D_0}\\): proportion of females from the source data\n\\(\\hat{p} = p_{F|D_f}\\): proportion of females from the fairface output\n\\(n = n_{F \\cup M|D_f}\\): number of data points in the gender subset form the fairface output\n\nAdditionally, we could test for different combinations of subsets within demographics. For instance, if we wanted to test for a statistically significant difference between the proportion of those who Female, given that they were Black, then we could write a hypothesis test like:\n\\(H_0: p_{F|B} = p_{F|D_0 \\cap B}\\)\n\\(H_A: p_{F|B} \\neq p_{F|D_0 \\cap B}\\)\nP-value Calculation:\n\\(P(|Z| &gt; \\frac{\\sqrt{n}(\\hat{p} - p)}{\\sqrt{p(1 - p)}})\\),\nwhere\n\n\\(p = p_{F|D_0 \\cap B}\\): proportion of females from the source data, given they were black\n\\(\\hat{p} = p_{F|D_f \\cap B}\\): proportion of females from the fairface output, given they were black\n\\(n = n_{F \\cup M|D_f \\cap B}\\): number of data points in the gender subset form the fairface output, given they were black.\n\nThese were two specific hypothesis tests, however, we’ll be testing many combinations of these parameters and reporting back on any significant findings.\n\n\n\n\n\n\nFrom the report requirements\n\n\n\nAlso can be called “Analyses”\nThis section might contain several subsections as needed.\n\nAt least one subsection should describe the exploratory data analysis you did.\nWhat modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)\nDescribe the analyses you did to answer the question of interest. Explain why you believe these methods are appropriate.\nAt least one subsection should describe the exploratory data analysis you did.\nWhat modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)\nDescribe the analyses you did to answer the question of interest. Explain why you believe these methods are appropriate.\nAt least one subsection should describe the exploratory data analysis you did.\nWhat modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)\nDescribe the analyses you did to answer the question of interest. Explain why you believe these methods are appropriate.\n\nSome methods we learn in this class include distribution comparison, correlation analysis, and hypothesis testing. You are required to include hypothesis tests into the project, but feel free to use additional methods to tell a good story about the data."
  },
  {
    "objectID": "methods.html#standardizing-output",
    "href": "methods.html#standardizing-output",
    "title": "3  Methods",
    "section": "3.4 Standardizing output",
    "text": "3.4 Standardizing output\nThe model outputs for both FairFace and DeepFace do not conform to the categories provided within the University of Tennessee - Knoxville (UTK) dataset. We elected to take the outputs from each model and modify them based upon the categories specified in the UTK dataset, namely:\n\n“[race] is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern).”\n“[gender] is either 0 (male) or 1 (female)”\n“[age] is an integer from 0 to 116, indicating the age”\n\n\n3.4.1 From FairFace\n\nRace: The FairFace classification model had two options - one for “fair7” and one for “fair4.” The latter provided predictions of race in the following categories: [White, Black, Asian, Indian]. Of key note, the model omitted “Other” categories as listed in the race category for the UTK dataset. However, the “fair7” model provides predictions across [White, Black, Latino_Hispanic, East Asian, Southeast Asian, Indian, Middle Eastern]. We elected to use the the fair7 model, and to refactor the output categories to match those of the UTK dataset. Namely, we refactored instances of Middle Eastern and Latino_Hispanic as “Other,” and instances of “East Asian” and “Southeast Asian” as “Asian”\nAge: FairFace only provides a predicted age range as opposed to a specific, single, predicted age as a string. To enable comparison of actual values to the predicted values, we maintained this column as a categorical variable, and split it into a lower and upper bound of predicted age as an integer. This split will allow us to determine whether or not the prediction correctly binned the age (i.e. \\(lowerBound \\leq actualAge \\leq upperBound\\)), and if not - how far outside of those bounds the actual age lay.\nGender: no change to outputs of “Male” and “Female.”\n\n\n\n3.4.2 From DeepFace\n\nRace: Racial categorical output from DeepFace includes the following categories [“middle eastern”, “asian”, “white”, “latino hispanic”, “black”, “indian”]\nAge: DeepFace provides a prediction of a single, specific, predicted age. We elected to match the predicted age to be the same range as would be predicted by Fair Face. For example, if DeepFace predicts an age like “19,” we assign it the same matching category as it would have in FairFace - “10-19.” From there, we also split this category into an upper and lower bound. In spite of the fact that DeepFace does not provide any bounds or ranges on its age prediction outputs, to have a similar and fair comparison of both models, we give it those same upper and lower bounds for equitable comparison.\nGender: DeepFace outputs are “Man” and “Woman”, and we refactor those values to “Male” and “Female” respectively."
  },
  {
    "objectID": "methods.html#evaluating-permutations-of-inputs-and-models-for-equitable-evaluation",
    "href": "methods.html#evaluating-permutations-of-inputs-and-models-for-equitable-evaluation",
    "title": "3  Methods",
    "section": "3.5 Evaluating Permutations of Inputs and Models for Equitable Evaluation",
    "text": "3.5 Evaluating Permutations of Inputs and Models for Equitable Evaluation\nAside from the differences in the outputs of each model in terms of age, race, and gender, there are also substantial differences between FairFace and DeepFace in terms of their available settings when attempting to categorize an image in each of these categories.\nThe need for this permutation evaluation rose from some initial scripting and testing of these models on a small sample of images from another facial dataset - the Asian Face Age Dataset (need citation here). We immediately grew concerned with DeepFace’s performance using default settings (namely, enforcing requirement to detect a face prior to categorization, and using OpenCV as the default detection backend). Running these initial scripting tests, we encountered a failure rate in DeepFace of approximately 70% in identifying and categorizing an image of a face.\nWe performed further exploratory analysis on both models in light of these facts, and sought some specific permutations of settings to determine what settings may provide the most fair and equitable comparison of the models prior to proceeding to further analysis.\n\n3.5.1 DeepFace Analysis Options\nDeepFace has a robust degree of avaialble settings when performing facial categorization and recognition. These include enforcing facial detection prior to classification of an image, as well as 8 different facial detection models to detect a face prior to categorization. The default of these settings is OpenCV detection with detection enabled. Other detection backends include ssd, dlib, mtcnn, retinaface, mediapipe, yolov8, yunet, and fastmtcnn.\nIn a Python 3.8 environment, attempting to run detections using dlib, retinaface, mediapipe, yolov8, and yunet failed to run, or failed to install the appropriate models directly from source during exeuction. Repairing any challenges or issues with the core functionality of DeepFace and FairFace’s code is outside the scope of our work, and as such, we have excluded any of these non-functioning models from our permutation evaluation.\n\n\n3.5.2 FairFace Analysis Options\nThe default script from FairFace provided no options via its command line script to change settings. It uses dlib/resnet34 models for facial detection and image pre-processing, and uses its own fair4 and fair7 models for categorization. There are no other options or flags that can be set by a user when processing a batch of images.\nWe converted the simple script to a class in Python without addressing any feature bugs or errors in the underlying code. This change provided us some additional options when performing the analysis of an input image using FairFace - namely, the ability to analyze and categorize an image with or without facial detection, similar to the functionality of DeepFace. FairFace remains limited in the fact that is only detection model backend is built in dlib, but this change gives us more options when considering what type of images to use and what settings to use on both models before generating our final dataset for analysis.\n\n\n3.5.3 Specific Permutations\nWith the above options in mind, we designed the following permutations for evaluation on a subset of the UTK dataset:\n\n\n\n\n\nDetection\nDetection Model\nImage Source\nResults Output\n\n\n\n\nEnabled\nFairFace=Dlib; DeepFace=OpenCV\nPre-cropped\nnew_ff_c_p.csv, crop_df_p_opencv.csv\n\n\nEnabled\nFairFace=Dlib; DeepFace=OpenCV\nIn-The-Wild\nnew_ff_uc_p.csv, uncropped_df_p_opencv.csv\n\n\nEnabled\nFairFace=Dlib; DeepFace=mtcnn\nPre-cropped\nnew_ff_c_p.csv, crop_df_p_mtcnn.csv\n\n\nEnabled\nFairFace=Dlib; DeepFace=mtcnn\nIn-The-Wild\nnew_ff_uc_p.csv, uncropped_df_p_mtcnn.csv\n\n\nDisabled\nFairFace,DeepFace=None\nPre-cropped\nnew_ff_c_np.csv, cropped_df_np.csv\n\n\nDisabled\nFairFace,DeepFace=None\nIn-The-Wild\nnew_ff_uc_np.csv, uncropped_df_np.csv\n\n\n\n\n\n\n\nWe processed each of the above setting permutations againnst approximately 9800 images, consisting of images from part 1 of 3 from the UTK datset. Each of the cropped images (cropped_UTK_dataset.csv) and uncropped images (uncropped_UTK_dataset.csv) came from the same underlying subject in each image; the only difference between each image was whether or not it was pre-processed before evaluation by each model.\n\n\n\n3.5.4 Permutation Sample Results (LN & DV)\n(enforcement of facial detection, detection backend model, and cropped images vs. faces in-the-wild)\n\n\n3.5.5 Setting Selection\n\nUpon completion of our evaluation, we determined the settings that gave both models the best chance of success included enabling facial detection with mtcnn for DeepFace and Dlib for FairFace on uncropped images.\nFrom there, we proceeded to process the entirety of the UTK dataset using these settings. The only exception are 4 images that did not conform to UTK’s naming convention to identify age, gender, and race of the subject in the image.\nWe wrote a script, MasterScript.py, to enable us to perform batch iteration of images and generate output files. When processing, we generated both the non-normalized output content and normalized output content.\nDue to the resource-intensive design of FairFace, our script enables multiprocessing of FairFace to allow for multiple simultaneous instances of the FairFace class as a pool of worker threads to iterate over all of the source data.\nWe attempted the same methodology for DeepFace, but encountered issues with silent errors and halting program execution when iterating over all images using DeepFace. To alleviate this challenge, we processed DeepFace in a single-threaded manner, and with smaller portions of the dataset vs. pursuing an all-in-one go execution. We proceeded to store the data for each of these smaller runs in multiple output files to combine once we completed all processing requirements.\nThe following table outlines the output files.\nThe last file, MasterDataFrame.csv, is the final output of our evaluation. This file is in the following format, with the following column definitions:\n\n\n\n\n\nColumn Name\nDefinition\nData Type\n\n\n\n\nimg_path\nRelative path location of the file within the UTK dataset\ncharacter vector\n\n\nfile\nThe filename of each file within the UTK dataset\ncharacter vector\n\n\nsrc_age\nThe age of the subject in each image from the UTK dataset\ninteger\n\n\nsrc_gender\nThe gender of the subject in each image from the UTK dataset\ncharacter vector\n\n\nsrc_race\nThe race of the subject in each image from the UTK datset\ncharacter vector\n\n\nsrc_timestamp\nThe time at which the image was submitted to the UTK dataset\ncharacter vector\n\n\nsrc_age_grp\nThe age group (matching age ranges from the FairFace outputs) for each image in the UTK dataset\ncharacter vector\n\n\npred_model\nThe model used to produce the predicted output (FairFace or DeepFace)\ncharacter vector\n\n\npred_race\nThe race of the subject in the image predicted by the given prediction model\ncharacter vector\n\n\npred_gender\nThe gender of the subject in the image predicted by the given prediction model\ncharacter vector\n\n\npred_age_DF_only\nThe integer-predicted age by DeepFace of the subject in the image\ninteger\n\n\npred_age_grp\nThe age group of the subject in the image predicted by the given prediction model\ncharacter vector\n\n\npred_age_lower\nThe integer lower bound of the predicted age group\ninteger\n\n\npred_age_upper\nThe integer upper bound of the predicted age group\ninteger\n\n\n\n\n\n\n\n\n\n\n\nKarkkainen, Kimmo, and Jungseock Joo. 2021. “FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation.” In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 1548–58."
  },
  {
    "objectID": "results.html#model-output",
    "href": "results.html#model-output",
    "title": "4  Results",
    "section": "4.1 Model Output",
    "text": "4.1 Model Output\nThe two models, DeepFace and FairFace, were run on the dataset described previously. In Figure 4.2, one can see the results of the predictions done by each model, by each factor that was considered: age, gender, and race. Note that the histogram distributions match the correct (source dataset) distributions, so we can see exactly the difference between what was provided and what was predicted, along with how well each model did on each category within each factor.\n\n\n\n\n\n\n\n(a) Gender predictions\n\n\n\n\n\n\n\n(b) Age predictions\n\n\n\n\n\n\n\n\n\n(c) Race predictions\n\n\n\n\nFigure 4.2: Histograms of the output from DeepFace and FairFace, with correct vs incorrect values colored. Note that the distributions match the correct (source dataset) distributions."
  },
  {
    "objectID": "results.html#model-performance",
    "href": "results.html#model-performance",
    "title": "4  Results",
    "section": "4.2 Model Performance",
    "text": "4.2 Model Performance\n\n4.2.1 TODO: Remove"
  },
  {
    "objectID": "results.html#hypothesis-testing",
    "href": "results.html#hypothesis-testing",
    "title": "4  Results",
    "section": "4.3 Hypothesis Testing",
    "text": "4.3 Hypothesis Testing\n\n4.3.1 TODO: Remove\n\n\n\n\n\n\nTest cateogry\nNull Proportion\nFairFace Proportion\nFairFace P-Value\nDeepFace Proportion\nDeepFace P-Value\n\n\n\n\n0-2\n0.0880010\n0.0762941\n0.0000000\n0.0000000\n0\n\n\n3-9\n0.0568832\n0.0664564\n0.0000000\n0.0000000\n0\n\n\n10-19\n0.0697867\n0.0606451\n0.0000000\n0.0206211\n0\n\n\n20-29\n0.3238735\n0.3480138\n0.0000000\n0.3987029\n0\n\n\n30-39\n0.1802755\n0.1762899\n0.1075659\n0.4047312\n0\n\n\n40-49\n0.0872542\n0.1023619\n0.0000000\n0.1467177\n0\n\n\n50-59\n0.0923160\n0.0930638\n0.6884418\n0.0268158\n0\n\n\n60-69\n0.0490831\n0.0490640\n0.9890528\n0.0024113\n0\n\n\n70-130\n0.0525268\n0.0278112\n0.0000000\n0.0000000\n0\n\n\n\n\n\n\n\n\n\n\n\n\nTest cateogry\nNull Proportion\nFairFace Proportion\nFairFace P-Value\nDeepFace Proportion\nDeepFace P-Value\n\n\n\n\nWhite\n0.4240727\n0.3854136\n0.0000000\n0.3757120\n0\n\n\nBlack\n0.1891129\n0.1652484\n0.0000000\n0.1479233\n0\n\n\nAsian\n0.1487843\n0.1448259\n0.0842652\n0.2408847\n0\n\n\nIndian\n0.1670816\n0.1030675\n0.0000000\n0.0611150\n0\n\n\nOther\n0.0709485\n0.2014445\n0.0000000\n0.1743649\n0\n\n\n\n\n\n\n\n\n\n\n\n\nTest cateogry\nNull Proportion\nFairFace Proportion\nFairFace P-Value\nDeepFace Proportion\nDeepFace P-Value\n\n\n\n\nFemale\n0.4780101\n0.4797642\n0.5857219\n0.3833202\n0\n\n\nMale\n0.5219899\n0.5202358\n0.5857219\n0.6166798\n0\n\n\n\n\n\n\n\n\n\n\n\n\nTest Category\nTest Condition\nNull Proportion\nFairFace Proportion\nFairFace P-Value\nDeepFace Proportion\nDeepFace P-Value\n\n\n\n\nFemale\nWhite\n0.4572938\n0.4888530\n0.0000000\n0.4355428\n3.32e-05\n\n\nFemale\nAsian\n0.5415505\n0.5431356\n0.8509504\n0.3879876\n0.00e+00\n\n\nFemale\nBlack\n0.4872751\n0.4649586\n0.0048467\n0.2405846\n0.00e+00\n\n\nFemale\nIndian\n0.4325801\n0.4430125\n0.2940541\n0.3496599\n0.00e+00\n\n\nFemale\nOther\n0.5508772\n0.4477643\n0.0000000\n0.3972341\n0.00e+00\n\n\nMale\nWhite\n0.5427062\n0.5111470\n0.0000000\n0.5644572\n3.32e-05\n\n\nMale\nAsian\n0.4584495\n0.4568644\n0.8509504\n0.6120124\n0.00e+00\n\n\nMale\nBlack\n0.5127249\n0.5350414\n0.0048467\n0.7594154\n0.00e+00\n\n\nMale\nIndian\n0.5674199\n0.5569875\n0.2940541\n0.6503401\n0.00e+00\n\n\nMale\nOther\n0.4491228\n0.5522357\n0.0000000\n0.6027659\n0.00e+00\n\n\n\n\n\n\n\n\n\n4.3.2 Updated Table Version with Data from Carl, Bhav\n\n4.3.2.1 TODO: Remove\n\n\n\n\n\nRace\nCategory\nF1_d\nF1_f\nAccuracy_d\nAccuracy_f\nprop_d\nprop_f\np_value_d\np_value_f\n\n\n\n\nAll\n0-2\nNA\n0.8959757\n0.5000000\n0.9172888\n0.0000000\n0.0762941\n0.0000000\n0.0000000\n\n\nAll\n3-9\nNA\n0.7176035\n0.5000000\n0.8772778\n0.0000000\n0.0664564\n0.0000000\n0.0000000\n\n\nAll\n10-19\n0.0478601\n0.5052498\n0.5055825\n0.7211461\n0.0206211\n0.0606451\n0.0000000\n0.0000000\n\n\nAll\n20-29\n0.5054326\n0.7332922\n0.6217793\n0.8050592\n0.3987029\n0.3480138\n0.0000000\n0.0000000\n\n\nAll\n30-39\n0.3786318\n0.4670003\n0.6275447\n0.6741504\n0.4047312\n0.1762899\n0.0000000\n0.1075659\n\n\nAll\n40-49\n0.2276278\n0.3943970\n0.5866155\n0.6786302\n0.1467177\n0.1023619\n0.0000000\n0.0000000\n\n\nAll\n50-59\n0.0801673\n0.4633983\n0.5137145\n0.7049843\n0.0268158\n0.0930638\n0.0000000\n0.6884418\n\n\nAll\n60-69\n0.0016129\n0.3739425\n0.4991769\n0.6708204\n0.0024113\n0.0490640\n0.0000000\n0.9890528\n\n\nAll\n70-130\nNA\n0.6270661\n0.5000000\n0.7383514\n0.0000000\n0.0278112\n0.0000000\n0.0000000\n\n\nAll\nWhite\n0.8095461\n0.8610399\n0.8365916\n0.8788455\n0.3757120\n0.3854136\n0.0000000\n0.0000000\n\n\nAll\nBlack\n0.7964994\n0.8684858\n0.8462797\n0.8997692\n0.1479233\n0.1652484\n0.0000000\n0.0000000\n\n\nAll\nAsian\n0.7038975\n0.8948932\n0.9005150\n0.9338128\n0.2408847\n0.1448259\n0.0000000\n0.0842652\n\n\nAll\nIndian\n0.4092481\n0.6402458\n0.6310597\n0.7488102\n0.0611150\n0.1030675\n0.0000000\n0.0000000\n\n\nAll\nOther\n0.2389021\n0.3087473\n0.6283106\n0.7105889\n0.1743649\n0.2014445\n0.0000000\n0.0000000\n\n\nAll\nFemale\n0.8197702\n0.9429153\n0.8402892\n0.9453080\n0.3833202\n0.4797642\n0.0000000\n0.5857219\n\n\nAll\nMale\nNA\nNA\nNA\nNA\n0.6166798\n0.5202358\n0.0000000\n0.5857219\n\n\nWhite\n0-2\nNA\n0.9039010\n0.5000000\n0.9334307\n0.0000000\n0.0737749\n0.0000000\n0.0000000\n\n\nWhite\n3-9\nNA\n0.7503392\n0.5000000\n0.8668432\n0.0000000\n0.0770059\n0.0000000\n0.0526039\n\n\nWhite\n10-19\n0.0634648\n0.5638298\n0.5102546\n0.7330315\n0.0163771\n0.0693592\n0.0000000\n0.0000000\n\n\nWhite\n20-29\n0.4256326\n0.6697460\n0.6363584\n0.8072440\n0.3311940\n0.2455574\n0.0000000\n0.0000000\n\n\nWhite\n30-39\n0.3884765\n0.4731553\n0.6486930\n0.6821608\n0.4022353\n0.1628433\n0.0000000\n0.0410025\n\n\nWhite\n40-49\n0.2224248\n0.3847156\n0.5730236\n0.6683039\n0.2100255\n0.1186861\n0.0000000\n0.0000001\n\n\nWhite\n50-59\n0.0890599\n0.4832502\n0.5086819\n0.7046059\n0.0376231\n0.1419494\n0.0000000\n0.0252436\n\n\nWhite\n60-69\nNaN\n0.3545817\n0.4978778\n0.6482054\n0.0025451\n0.0680668\n0.0000000\n0.0064930\n\n\nWhite\n70-130\nNA\n0.6342183\n0.5000000\n0.7403000\n0.0000000\n0.0427571\n0.0000000\n0.0000000\n\n\nWhite\nMale\n0.8892356\n0.9595281\n0.8697687\n0.9566327\n0.5644572\n0.5111470\n0.0000332\n0.0000000\n\n\nWhite\nFemale\n0.8556585\n0.9526238\n0.8697687\n0.9566327\n0.4355428\n0.4888530\n0.0000332\n0.0000000\n\n\nAsian\n0-2\nNA\n0.9164589\n0.5000000\n0.9301909\n0.0000000\n0.2029235\n0.0000000\n0.0000018\n\n\nAsian\n3-9\nNA\n0.7140255\n0.5000000\n0.9111790\n0.0000000\n0.0928633\n0.0000000\n0.0000000\n\n\nAsian\n10-19\n0.0395257\n0.3798450\n0.5023622\n0.6944844\n0.0457370\n0.0366867\n0.0000000\n0.2105900\n\n\nAsian\n20-29\n0.5572885\n0.8557951\n0.5947638\n0.8792496\n0.5044874\n0.4379478\n0.0000000\n0.0001071\n\n\nAsian\n30-39\n0.2992611\n0.5069357\n0.6258649\n0.7042053\n0.3206766\n0.0988822\n0.0000000\n0.0000180\n\n\nAsian\n40-49\n0.1520190\n0.3320463\n0.5924407\n0.6626378\n0.0995858\n0.0369733\n0.0000000\n0.3360992\n\n\nAsian\n50-59\n0.0898876\n0.4608696\n0.5273304\n0.7272357\n0.0250259\n0.0315277\n0.0066069\n0.9201820\n\n\nAsian\n60-69\nNaN\n0.4141414\n0.4988555\n0.7581786\n0.0044874\n0.0315277\n0.0000000\n0.0000225\n\n\nAsian\n70-130\nNA\n0.7441860\n0.5000000\n0.8051278\n0.0000000\n0.0306678\n0.0000000\n0.0000000\n\n\nAsian\nMale\n0.7940330\n0.8914286\n0.7911354\n0.8993780\n0.6120124\n0.4568644\n0.0000000\n0.8509504\n\n\nAsian\nFemale\n0.7630232\n0.9058670\n0.7911354\n0.8993780\n0.3879876\n0.5431356\n0.0000000\n0.8509504\n\n\nBlack\n0-2\nNA\n0.8854962\n0.5000000\n0.9026663\n0.0000000\n0.0180859\n0.0000000\n0.2466380\n\n\nBlack\n3-9\nNA\n0.7400881\n0.5000000\n0.8957332\n0.0000000\n0.0298920\n0.0000000\n0.0039351\n\n\nBlack\n10-19\n0.0164609\n0.3784787\n0.5032694\n0.6953003\n0.0000000\n0.0635519\n0.0000000\n0.0003683\n\n\nBlack\n20-29\n0.5880567\n0.6875152\n0.6367203\n0.7184594\n0.4094997\n0.4687265\n0.0272015\n0.0000002\n\n\nBlack\n30-39\n0.4413203\n0.4518681\n0.5975793\n0.6299581\n0.4724564\n0.2398895\n0.0000000\n0.0001798\n\n\nBlack\n40-49\n0.1827542\n0.3260274\n0.5549318\n0.6301481\n0.1017426\n0.0864104\n0.0000300\n0.3688694\n\n\nBlack\n50-59\n0.0549451\n0.3565062\n0.5097384\n0.6523810\n0.0160202\n0.0557649\n0.0000000\n0.0421863\n\n\nBlack\n60-69\n0.0104712\n0.3508772\n0.5019317\n0.6526201\n0.0002811\n0.0301432\n0.0000000\n0.0012999\n\n\nBlack\n70-130\nNA\n0.4086022\n0.5000000\n0.6383490\n0.0000000\n0.0075358\n0.0000000\n0.0000000\n\n\nBlack\nMale\n0.8471279\n0.9637681\n0.8126869\n0.9625782\n0.7594154\n0.5350414\n0.0000000\n0.0048467\n\n\nBlack\nFemale\n0.7724665\n0.9615732\n0.8126869\n0.9625782\n0.2405846\n0.4649586\n0.0000000\n0.0048467\n\n\nIndian\n0-2\nNA\n0.8132911\n0.5000000\n0.8442303\n0.0000000\n0.0318164\n0.0000000\n0.0000000\n\n\nIndian\n3-9\nNA\n0.6047619\n0.5000000\n0.8704983\n0.0000000\n0.0535642\n0.0000000\n0.0012086\n\n\nIndian\n10-19\nNaN\n0.4719764\n0.4916753\n0.7130687\n0.0006803\n0.0543697\n0.0000000\n0.0164656\n\n\nIndian\n20-29\n0.4985183\n0.7635997\n0.5964419\n0.8073113\n0.3346939\n0.3407169\n0.0000262\n0.0000012\n\n\nIndian\n30-39\n0.3380175\n0.4403292\n0.5981267\n0.6658402\n0.5401361\n0.2219090\n0.0000000\n0.0000000\n\n\nIndian\n40-49\n0.2997904\n0.4636872\n0.6063907\n0.7199526\n0.1170068\n0.1494160\n0.7150347\n0.0000000\n\n\nIndian\n50-59\n0.0581655\n0.4649351\n0.5119443\n0.6957273\n0.0074830\n0.0841724\n0.0000000\n0.0073901\n\n\nIndian\n60-69\nNaN\n0.4563758\n0.4997424\n0.7281110\n0.0000000\n0.0447040\n0.0000000\n0.0107443\n\n\nIndian\n70-130\nNA\n0.5882353\n0.5000000\n0.7307264\n0.0000000\n0.0193315\n0.0000078\n0.0103013\n\n\nIndian\nMale\n0.8897860\n0.9574045\n0.8485028\n0.9525160\n0.6503401\n0.5569875\n0.0000000\n0.2940541\n\n\nIndian\nFemale\n0.8234153\n0.9452171\n0.8485028\n0.9525160\n0.3496599\n0.4430125\n0.0000000\n0.2940541\n\n\nOther\n0-2\nNA\n0.9193246\n0.5000000\n0.9396536\n0.0000000\n0.0605811\n0.0000000\n0.0000000\n\n\nOther\n3-9\nNA\n0.7043189\n0.5000000\n0.8634606\n0.0000000\n0.0638780\n0.0000000\n0.0000091\n\n\nOther\n10-19\n0.0597015\n0.5460317\n0.4968047\n0.7189416\n0.0195517\n0.0620235\n0.0000000\n0.0000000\n\n\nOther\n20-29\n0.4800507\n0.7610619\n0.5252573\n0.8012345\n0.4113019\n0.3840923\n0.1160161\n0.0292985\n\n\nOther\n30-39\n0.3505618\n0.5017182\n0.6391325\n0.7224076\n0.4213162\n0.1821554\n0.0000000\n0.0000000\n\n\nOther\n40-49\n0.3068783\n0.4630542\n0.6199385\n0.7061024\n0.1239866\n0.1071502\n0.0000000\n0.0000000\n\n\nOther\n50-59\n0.0983607\n0.5000000\n0.5278998\n0.7472707\n0.0219361\n0.0789203\n0.0091047\n0.0000000\n\n\nOther\n60-69\nNA\n0.6000000\n0.5000000\n0.8318627\n0.0019075\n0.0430661\n0.0008174\n0.0000000\n\n\nOther\n70-130\nNA\n0.5000000\n0.5000000\n0.6666667\n0.0000000\n0.0181331\n0.0001216\n0.0000000\n\n\nOther\nMale\n0.8216482\n0.9047310\n0.8341205\n0.9136183\n0.6027659\n0.5522357\n0.0000000\n0.0000000\n\n\nOther\nFemale\n0.8379888\n0.9216000\n0.8341205\n0.9136183\n0.3972341\n0.4477643\n0.0000000\n0.0000000"
  },
  {
    "objectID": "results.html#model-performance-hypothesis-testing",
    "href": "results.html#model-performance-hypothesis-testing",
    "title": "4  Results",
    "section": "4.4 Model Performance, Hypothesis Testing",
    "text": "4.4 Model Performance, Hypothesis Testing\nFor each category and model, we calculate the F1 score, accuracy, and p-value, as described in section 3. The results are summarized in ?tbl-perf-pvalue. Cell values are colored according to the strength of the metric.\nWe also specifically looked at the performance metrics of the models, when controlled for specific race groups;\n\n4.4.1 TODO\n\nAdd color key\nColor p-values based on sig level = 99.7%\nBetter description of signifiance of numerical values of Accuracy, F1 score\nAdd line plot of F1, Accuracy, p-value OR correlation matrix\nMake table caption work?\n\nStatic table:\n\nValues where we FAIL to reject null hypothesis\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.2 Statistical Power\n\\[\n\\beta = P\\bigg(\\bigg|\\frac{\\sqrt{n}\\cdot\\hat{p}-p_a}{\\sqrt{p_a(1-p_a)}}\\bigg|\\geq\\frac{\\sqrt{n}\\cdot p_0-p_a}{\\sqrt{p_0(1-p_0)}}\\bigg)\n\\]\nOur selected level of significance is 99.7% (3-sigma). Type-II error is denoted by \\(\\beta\\) above, and Power will be \\(1-\\beta\\)\nWith \\(p_0\\) being our assumed population proportion (from the source dataset and what we used in our tests), \\(p_a\\) being the actual population proportion (from one or more of the below methods), \\(n\\) being the number of predicted members of a racial group (i.e. “Indian”),\n\nFor Gender - assume that sex at birth is a bernoulli trial, over time, the proportion for both genders should be 0.5\nFor age groups - assume that age has a true normal distribution. Each race may have different means and standard deviations for their distribution of age, but still adhere to a normal distribution. The “population” proportions may be a bit more challenging to calculate, but under this framework, we may be able to get there.\n\nMay be able to get via bootstrapping the source dataset, average age by race - I think that’s what we did in our last project?\nCould look at external data? May not have time to look through everything.\n\n\n\n\\[\n\\frac{\\sqrt{n_M}\\cdot(\\bar{p}_M-p_S)}{\\sqrt{p_S\\cdot(1-P_S)}}\n\\]"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "5  Conclusions",
    "section": "",
    "text": "From the report requirements\n\n\n\n\nSummarize what the paper has done, and discuss the implications of your Results.\nExplicitly connect the results to the research question.\nDiscuss how you would you extend this research\n\nLike the introduction, this section should be written with a non-expert in mind. A person should be able to read Introduction+Conclusion and get a rough idea of the meaning and significance of your paper"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Karkkainen, Kimmo, and Jungseock Joo. 2021. “FairFace: Face\nAttribute Dataset for Balanced Race, Gender, and Age for Bias\nMeasurement and Mitigation.” In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, 1548–58.\n\n\n“UTKFace.” 2021. UTKFace. https://susanqq.github.io/UTKFace."
  }
]