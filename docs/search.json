[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bias in Facial Classification ML Models",
    "section": "",
    "text": "Abstract\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\nFeatures of Quarto:"
  },
  {
    "objectID": "index.html#how-we-should-write-this-report",
    "href": "index.html#how-we-should-write-this-report",
    "title": "Bias in Facial Classification ML Models",
    "section": "How we should write this report",
    "text": "How we should write this report\n\nSee Karkkainen and Joo (2021) , that is an example on how to cite a bibliography.\nSections/title headings are automatically numbered.\nAny changes you make, make sure to make a comment of your initials at the top of your work (INCLUDING written text) like so:\n\n&lt;!-- BJ !--&gt;\nBlah blah etc ....\n\nOR\n#BJ\nr_var &lt;- ...\n\nMake sure to add a unique name to all code cells, and to also enable the following (the quarto way) (In order for a figure to be cross-referenceable, its label must start with the fig- prefix):\n\n\n\n\n\n\nFigure 1: A caption for generated figure\n\n\n\n\n\nYou can then refer to figures like this @fig-sec1-unique-name Figure 1\nFormat tables doing the following Link here\nDo all your r work initially in your own custom .rmd file in this directory, so that it can be copy-pasted over later into the appropriate section (written descriptions/words can go straight into the .qmd files though). For example, Bhav’s work is in 5000-final/BJ_work.rmd.\n\n\n\n\n\n\n\nFrom the report requirements\n\n\n\nA 3-5 summary of the paper. It should address the research question, the methods, and the conclusions of your analysis.\n“A good recipe for an abstract is: first sentence: specify the general area of the paper and encourage the reader; second sentence: specify the dataset and methods at a general level; third sentence: specify the headline result; and a fourth sentence about implications.”\n\n\n\n\n\n\nKarkkainen, Kimmo, and Jungseock Joo. 2021. “FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation.” In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 1548–58."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "From the report requirements\n\n\n\nThis section introduces your problem to a non-expert audience, describes the context and history of the problem.\nFor example, if your overall project topic is on Diabetes Prevention and Prediction, then you would use the Introduction to introduce what diabetes is, who it affects, why prevention is important, history on diabetes prevention, etc.\nSome questions that you could answer in the introduction:\n\nWhat is the “research question”? why is it interesting or worth answering?\nWhat is the relevant background information for readers to understand your project? Assume that your audience\nis not an expert in the application field.\nIs there any prior research on your topic that might be helpful for the audience?\n\nThe goal of the introduction it to capture the audience’s interest in your paper. An introduction that starts with “Diabetes kills over 87 thousand people each year and in many cases may be preventable” is more engaging than “This paper is about diabetes prevention”.\nThe introduction should be 2-4 paragraphs long."
  },
  {
    "objectID": "data.html#exploration-of-source-data",
    "href": "data.html#exploration-of-source-data",
    "title": "2  Data",
    "section": "2.1 Exploration of Source Data",
    "text": "2.1 Exploration of Source Data\n\n\n\n\n\n\n\n(a) Age=6, Gender=F, Race=Indian\n\n\n\n\n\n\n\n(b) Age=38, Gender=M, Race=White\n\n\n\n\n\n\n\n(c) Age=80, Gender=M, Race=Asian\n\n\n\n\nFigure 2.1: Example face images from the UTK dataset (“UTKFace” 2021) with their associated given labels.\n\n\n\n\n\n\nFigure 2.2: An interactive figure showcasing the distributions of various data factors in the image dataset, and showcasing the underlying data.\n\n\n\n\n\n\n\n\nFrom the report requirements\n\n\n\nThis section should describe the data you’ll be using. Answer at least all of the following questions:\n\nHow was the data collected?\n\nThe dataset used in this research is a publicly non-commercial available dataset on Github called “UTKFace”. The data was collected by the The University of Tennessee, Knoxville. It is specified on its Github page that the images were gathered from the internet. They are likely to be obtained through technique such as web scrapping. The dataset contains more than 20,000 images, representing a highly diversed demographic. However, face images are vary in pose, facial expression, lighting, and resolution.\n\nWhat are the sources and influences of bias in the data?\n\nThe distribution of each demographic groups are not normally distributed. By plotting a distribution of each demographic group, it is evident that the dataset contains an uneven high volume of older White men. While a smaller porportion of female among …(input race)… is present.\n\nWhat are the important features (=columns) that you are using in your analysis? What do they mean?\n\nThere are three features in the dataset which are essential to our analysis. They are Race, Gender, and Age.\nRace is categorized into five groups; Asian, Black, Indian, White, and Other. It should be noted by Asian group in this dataset mostly refers to people from East and Southeast Asia. Whereas, Other includes ethnicities such as Hispanic, Latino, and Middle Eastern.\nGender is divided into two groups, either male or female.\nLastly, Age is represented with an integer. This dataset contains people of all ages ranging from 0 to 116.\nFeel free to add anything else that you think is necessary for understanding the paper and the context of the problem."
  },
  {
    "objectID": "data.html#data-selection-utk-dataset---ln-dv",
    "href": "data.html#data-selection-utk-dataset---ln-dv",
    "title": "2  Data",
    "section": "2.2 Data Selection (UTK Dataset) - LN DV",
    "text": "2.2 Data Selection (UTK Dataset) - LN DV\nMotivation - has there been progress against bias?\nReference Articles from Ethics Class - Joy B’s work, etc. Lots of disparity back in 2018, how much of that still exists with free models?\nIn 2018, Joy Buolamwini, a PhD candinate at MIT Media Lab, published a thesis on gender and racial biases in facial recognition in algorithms. In her paper, she tested facial recongition softwares from multiple large technology companies such as Microsoft, IBM, and Amazom on its effectiveness for different demographic groups. From this research, it is concluded that most AI algorithms offer a substantially less accurate prediction for female and people with dark skin color.\n(Add link to the story)\nThe primary motivation behind this research is to determine the degree in which bias is still present in modern facial recognition models. Therefore, we select a dataset which comprise of face images with high diversity in regards to race."
  },
  {
    "objectID": "data.html#selected-models-ln-dv",
    "href": "data.html#selected-models-ln-dv",
    "title": "2  Data",
    "section": "2.3 Selected Models (LN DV)",
    "text": "2.3 Selected Models (LN DV)\n\n2.3.1 FairFace\nDeveloped by researchers at University of California, Los Angeles, FairFace was specifically designed to mitigate gender and racial biases. The model was trained on 100K+ face images of people of various ethnicities with normal distribution (equal stratification) across all groups. Beside facial recognition model, FairFace also provided the dataset which it was trained on. The dataset is highly popular among\nWe came across this model when we were searching for an\n\n\n2.3.2 DeepFace\nDeepFace is a lightweight open-source model developed and used by Meta (Facebook). Since the model is used by one of the largest social media company, it is widely known among developers. Therefore, its popularity prompts us to evaluate its performance. It should be noted that this model of DeepFace is a free open-source version. It is highly likely that this version is less advanced than what Meta is actually using. Thus, we should not view the result of this model as a representative of Meta’s algorithm."
  },
  {
    "objectID": "data.html#dataset-features",
    "href": "data.html#dataset-features",
    "title": "2  Data",
    "section": "2.4 Dataset Features",
    "text": "2.4 Dataset Features\nUnderstanding and selecting the appropriate features for our data is key to success in analysis. Furthermore, understanding feature differences and planning standardization across datasets is key in making sound comparisons and analyses upon the dataset.\n\n2.4.1 Input data set\nThe input dataset, being UTKFace, provided feature information natively in each filename without additional external data. The features contained therein include the following items for each image’s subject. They are defined as follows in the UTKFace Readme:\n\n“[race] is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern).”\n“[gender] is either 0 (male) or 1 (female)”\n“[age] is an integer from 0 to 116, indicating the age”\n\nWe processed each image to extract these features from each image and create a table of source information (link to source data file here).\nAs our work is focused in potential biases in protected classes such as race, gender, and age, the features of UTKFace are sufficient to meet the needs for an input dataset for category prediction in our selected models.\n\n\n2.4.2 FairFace Outputs\nFairFace outputs provided predictions age and race, and two different predictions for race - one based upon their “Fair4” model, and the other based upon their “Fair7” model. In addition to these predictions, the output included scores for each category. With the nature of our planned analyses, the scores are of less import to us in our evaluation.\nTo examine more in detail on “Fair” and “Fair4” models, the latter provided predictions of race in the following categories: [White, Black, Asian, Indian]. Of key note, the “Fair4” model omitted “Other” categories as listed in the race category for the UTK dataset. However, the “Fair7” model provides predictions across [White, Black, Latino_Hispanic, East Asian, Southeast Asian, Indian, Middle Eastern]. We elected to use the the Fair7 model, and to refactor the output categories to match those of the UTK dataset. Namely, we refactored instances of Middle Eastern and Latino_Hispanic as “Other” and instances of “East Asian” and “Southeast Asian” as “Asian” to match the categories explicitly listed in UTKFace.\nAdditionally, FairFace only provides a predicted age range as opposed to a specific, single, predicted age as a string. To enable comparison of actual values to the predicted values, we maintained this column as a categorical variable, and split it into a lower and upper bound of predicted age as an integer in the event we require it for our analyses.\nWith the above considerations in mind, the following output features are of import to the team:\n\n\n\n\n\n\n\n\n\n\n\nColumn Name\nData Type\nSignificance\nValid Values\n\n\n\n\nname_face_align\nString\nThe name and path of the file upon which FairFace made predictions\n[filepath]\n\n\nrace_preds_fair\nString\nThe predicted race of the image subject\n[White|Black|Latino_Hispanic|East Asian|Southeast Asian|Middle Eastern|Indian]\n\n\ngender_preds_fair7\nString\nThe predicted gender of the image subject\n[Male|Female]\n\n\nage_preds_fair\nString\nThe predicted age range of the image subject\n[‘0-2’|‘3-9’|‘10-19’|‘20-29’|‘30-39’|‘40-49’|‘50-59’|‘60-69’|‘70+’]\n\n\n\n\n\n\n\n2.4.3 DeepFace Outputs\nDefault outputs have a wide-range of information for the user. In addition to providing its predictions, DeepFace also provides scores associated with each evaluation on a per class basis (i.e. 92% for Race #1, 3% Race #2, 1% Race #3, and 4% Race #4). For the purpose of our planned analyses, the score features are of less concern to us.\nWe hone in on the following select features from DeepFace outputs to have the ability to cross-compare between UTKFace, FairFace, and DeepFace:\n\n\n\n\n\n\n\n\n\n\n\nColumn Name\nData Type\nSignificance\nValid Values\n\n\n\n\nAge\nInteger\nThe predicted age of the image subject\nAny Integer\n\n\nDominant Gender\nString\nThe predicted gender of the iamge subject\n[Man|Woman]\n\n\nDominant Race\nString\nThe predicted race of the image subject\n[middle eastern|asian|white|latino hispanic|black|indian]\n\n\n\n\n\n\n\n2.4.4 Standardizing model outputs\nAs can be seen above, there are some key differences between the outputs of both models as well as the source data that we needed to resolve to enable comparison of each dataset to one another. We’ll focus on the primary features of age, gender, and race from each dataset.\n\n2.4.4.1 FairFace Output Modifications\nWe’ll discuss FairFace first, as it introduces a requirement for modification to both our input information as well as the outputs for DeepFace.\nAge - FairFace only provides a categorical predicted age range as opposed to a specific numeric age. We retain this age format and modify the last category of “70+” to “70-130” to ensure we can capture the gamut of all input and output ages in all datasets.\nGender - No changes to predicted values; use “Male” and “Female”\nRace - the source data from UTKFace has 5 categories “White” “Black” “Asian” “Indian” and “Other”. Using the defintions from UTKFace, we collapse the output categories of FairFace’s Fair7 model as follows:\n[“Southeast Asian”,“East Asian”] =&gt; “Asian” [“Middle Eastern” , “Latino_Hispanic] =&gt;”Other”\n\n\n\n2.4.5 DeepFace Output Modifications\nAge Cut the predicted age into bins based upon the same prediction ranges provided by FairFace. If the DeepFace predicted age falls into a range provided by FairFace, provide that as the predicted age range for DeepFace.\nGender: we adjust the DeepFace gender prediction outputs to match that of the source and FairFace data with the following refactoring: “Man” =&gt; “Male” “Woman” =&gt; “Female”\nRace: we adjust the DeepFace race prediction outputs to match that of the source dataset with the following refactoring:\n\n“white” =&gt; “White”\n“black” =&gt; “Black”\n“indian” =&gt; “Indian”\n“asian” =&gt; “Asian”\n[“middle eastern”, “latino hispanic”] =&gt; “Other”\n\n\n\n2.4.6 Source Data Modifications\nAge: We cut the predicted age into bins based upon the same prediction ranges provided by FairFace. If the input / source data age falls into a range provided by FairFace, provide that is the source age range for the image subject.\nGender: No changes.\nRace: No changes."
  },
  {
    "objectID": "data.html#evaluating-permutations-of-inputs-and-models-for-equitable-evaluation",
    "href": "data.html#evaluating-permutations-of-inputs-and-models-for-equitable-evaluation",
    "title": "2  Data",
    "section": "2.5 Evaluating Permutations of Inputs and Models for Equitable Evaluation",
    "text": "2.5 Evaluating Permutations of Inputs and Models for Equitable Evaluation\nAside from the differences in the outputs of each model in terms of age, race, and gender, there are also substantial differences between FairFace and DeepFace in terms of their available settings when attempting to categorize and predict the features associated with an image.\nThe need for this permutation evaluation rose from some initial scripting and testing of these models on a small sample of images from another facial dataset - the Asian Face Age Dataset (need citation here). We immediately grew concerned with DeepFace’s performance using default settings (namely, enforcing requirement to detect a face prior to categorization/prediction, and using OpenCV as the default detection backend). Running these initial scripting tests, we encountered a face detection failure rate, and thus a prediction failure rate, in DeepFace of approximately 70%.\nWe performed further exploratory analysis on both models in light of these facts, and sought some specific permutations of settings to determine what settings may provide the most fair and equitable comparison of the models prior to proceeding to further analysis.\nThe ultimate goal for us in performing this exploration was to identify the settings for each model that might best increase the likelihood that the model’s output would result in a failure to reject our null hypotheses. In lamens terms, our tests sought out the combination of settings that give each model the benefit of the doubt, and for each to deliver the greatest accuracy in their predictions. For simplicity’s sake, we leaned solely on the proportion of true positives across each category when compared with the source information to decide which settings to use.\n\n2.5.1 DeepFace Analysis Options\nDeepFace has a robust degree of avaialble settings when performing facial categorization and recognition. These include enforcing facial detection prior to classification of an image, as well as 8 different facial detection models to detect a face prior to categorization. The default of these settings is OpenCV detection with detection enabled. Other detection backends include ssd, dlib, mtcnn, retinaface, mediapipe, yolov8, yunet, and fastmtcnn.\nIn a Python 3.8 environment, attempting to run detections using dlib, fastmtcnn, retinaface, mediapipe, yolov8, and yunet failed to run, or failed to install the appropriate models directly from source during exeuction. Repairing any challenges or issues with the core functionality of DeepFace and FairFace’s code is outside the scope of our work, and as such, we have excluded any of these non-functioning models from our settings permutation evaluation.\n\n\n2.5.2 FairFace Analysis Options\nThe default script from FairFace provided no options via its command line script to change runtime settings. It uses dlib/resnet34 models for facial detection and image pre-processing, and uses its own Fair4 and Fair7 models for categorization. There are no other options or flags that can be set by a user when processing a batch of images.\nWe converted the simple script to a class in Python without addressing any feature bugs or errors in the underlying code. This change provided us some additional options when performing the analysis of an input image using FairFace - namely, the ability to analyze and categorize an image with or without facial detection, similar to the functionality of DeepFace. FairFace remains limited in the fact that is only detection model backend is built in dlib, but this change from a script to a class object gave us more options when considering what type of images to use and what settings to use on both models before generating our final dataset for analysis.\n\n\n2.5.3 Specific Permutations\nWith the above options in mind, we designed the following permutations for evaluation on a subset of the UTK dataset:\n\n\n\n\n\nDetection\nDetection Model\nImage Source\n\n\n\n\nEnabled\nFairFace=Dlib; DeepFace=OpenCV\nPre-cropped\n\n\nEnabled\nFairFace=Dlib; DeepFace=OpenCV\nIn-The-Wild\n\n\nEnabled\nFairFace=Dlib; DeepFace=mtcnn\nPre-cropped\n\n\nEnabled\nFairFace=Dlib; DeepFace=mtcnn\nIn-The-Wild\n\n\nDisabled\nFairFace,DeepFace=None\nPre-cropped\n\n\nDisabled\nFairFace,DeepFace=None\nIn-The-Wild\n\n\n\n\n\n\n\nWe processed each of the above setting permutations against approximately 9800 images, consisting of images from part 1 of 3 from the UTK datset. Each of the cropped images (cropped_UTK_dataset.csv) and uncropped images (uncropped_UTK_dataset.csv) came from the same underlying subject in each image; the only difference between each image was whether or not it was pre-processed before evaluation by each model. Having the same underlying source subject enables us to perform a direct comparison of results between cropped vs. in-the-wild images, and better support a conlcusion of which settings to use.\n\n\n\n\n\npred_model\ndetection_enabled\ndetection_model\nimage_type\nall_rate\nage_grp_rate\ngender_rate\nrace_rate\n\n\n\n\nDeepFace\nFalse\nNone\ncropped\n0.0724949\n0.1601227\n0.6667689\n0.6951943\n\n\nDeepFace\nFalse\nNone\nuncropped\n0.0834356\n0.1522495\n0.7326176\n0.6457055\n\n\nDeepFace\nTrue\nmtcnn\ncropped\n0.0889571\n0.1534765\n0.7249489\n0.6807771\n\n\nDeepFace\nTrue\nmtcnn\nuncropped\n0.1023517\n0.1615542\n0.7834356\n0.6665644\n\n\nDeepFace\nTrue\nopencv\ncropped\n0.0267894\n0.0765849\n0.1887526\n0.1983640\n\n\nDeepFace\nTrue\nopencv\nuncropped\n0.0806748\n0.1455010\n0.6619632\n0.5855828\n\n\nFairFace\nFalse\nNone\ncropped\n0.4015337\n0.6101227\n0.8921268\n0.7689162\n\n\nFairFace\nFalse\nNone\nuncropped\n0.1031697\n0.2671779\n0.7599182\n0.4477505\n\n\nFairFace\nTrue\ndlib\ncropped\n0.4015337\n0.6101227\n0.8921268\n0.7689162\n\n\nFairFace\nTrue\ndlib\nuncropped\n0.4353783\n0.6230061\n0.9155419\n0.7914110\n\n\n\n\n\n\n\nExamining the true positive ratios for each case, our team came to the conclusion that the settings that gave both models the best chance for success in correctly predicting the age, gender, and race of subject images are as follows:\n\nFairFace: enforce facial detection with dlib, and use uncropped images for evaluation\nDeepFace: enforce facial detection with MTCNN detection backend, and use uncropped images for evaluation.\n\nThese settings are equitable and make a degree of sense. Using facial detection, specifically-coded for each model, should give each model the ability to isolate the portions of a face necessary for them to make a prediction, as opposed to using a pre-cropped image that could include unneeded information, or excluded needed information.\nHaving decided on these settings, our team proceeded to run the entirety of the UTK dataset through both DeepFace and FairFace models using a custom coded script MasterScript.py that allowed us to apply multiprocessing across the list of images and evaluate all items in a reasonable amount of time. (cite FairFace here, may also need to reference that we rebuilt their script into a class format).\nDue to the resource-intensive design of FairFace, our script enables multiprocessing of FairFace to allow for multiple simultaneous instances of the FairFace class as a pool of worker threads to iterate over all of the source data.\nWe attempted the same multiprocessing methodology for DeepFace, but encountered issues with silent errors and halting program execution when iterating over all images using DeepFace. To alleviate this challenge, we processed DeepFace in a single-threaded manner, and with smaller portions of the dataset vs. pursuing an all-in-one go execution. We proceeded to store the data for each of these smaller runs in multiple output files to combine once we completed all processing requirements."
  },
  {
    "objectID": "data.html#result-output-format",
    "href": "data.html#result-output-format",
    "title": "2  Data",
    "section": "2.6 Result Output Format",
    "text": "2.6 Result Output Format\nThe following table outlines, after the input and output data modifications, the final format of our data for use in our analyses. This file is stored in the /data folder as MasterDataFrame.csv\n\n\n\n\n\nColumn Name\nDefinition\nData Type\n\n\n\n\nimg_path\nRelative path location of the file within the UTK dataset\ncharacter vector\n\n\nfile\nThe filename of each file within the UTK dataset\ncharacter vector\n\n\nsrc_age\nThe age of the subject in each image from the UTK dataset\ninteger\n\n\nsrc_gender\nThe gender of the subject in each image from the UTK dataset\ncharacter vector\n\n\nsrc_race\nThe race of the subject in each image from the UTK datset\ncharacter vector\n\n\nsrc_timestamp\nThe time at which the image was submitted to the UTK dataset\ncharacter vector\n\n\nsrc_age_grp\nThe age group (matching the predicted age ranges from the FairFace outputs) for each image in the UTK dataset\ncharacter vector\n\n\npred_model\nThe model used to produce the predicted output (FairFace or DeepFace)\ncharacter vector\n\n\npred_race\nThe race of the subject in the image, predicted by the given prediction model under the pred_model column\ncharacter vector\n\n\npred_gender\nThe gender of the subject in the image, predicted by the given prediction model under the pred_model column\ncharacter vector\n\n\npred_age_DF_only\nThe integer-predicted age by DeepFace of the subject in the image\ninteger\n\n\npred_age_grp\nThe age group of the subject in the image, predicted by the given prediction model under the pred_model column\ncharacter vector\n\n\npred_age_lower\nThe integer lower bound of the predicted age group\ninteger\n\n\npred_age_upper\nThe integer upper bound of the predicted age group\ninteger\n\n\n\n\n\n\n\n\n\n\n\n“UTKFace.” 2021. UTKFace. https://susanqq.github.io/UTKFace."
  },
  {
    "objectID": "methods.html#the-big-picture",
    "href": "methods.html#the-big-picture",
    "title": "3  Methods",
    "section": "3.1 The Big Picture",
    "text": "3.1 The Big Picture\n\nIs bias prevalent in facial recognition machine learning models?\nCan one model be shown to have statistically significant less bias than the other?\nDoes one model outperform the other in a statistically significant manner, in all aspects?\nDoes one model outperform the other in a statistically significant manner, in certain aspects?\n\nThis is where we can dive into “conventional” bias\n\nAre there disparate outcomes (i.e. lower chances of correct classification) for one racial group vs. another?\n\n\n\n\n\n\n\nThoughts on Bias\n\n\n\nWe need to be careful how we define and use bias. Statistical bias is essentially error, and we could be crossing our definitions between statistical bias and conventional bias."
  },
  {
    "objectID": "methods.html#measuring-performance",
    "href": "methods.html#measuring-performance",
    "title": "3  Methods",
    "section": "3.2 Measuring Performance",
    "text": "3.2 Measuring Performance\n\n\n\n\n\n\nNote\n\n\n\nThis performance section is important in choosing the correct models to ensure data integrity, however for the actual statistical tests, we’ll focused on more common statistics like mean and proportion.\n\nThese are my recommendations on how we can look at the additional values:\nFor cases in which we reject the statistical null hypothesis, we plan to evaluate the below metrics for the same output category. Should we reject a null hypothesis for, let’s say, the proportion of females, given that they are asian, we will examine the accuracy and F-1 scores for the same category. We will consider the following range of values:\n\nAccuracy &lt; 70: poor/unacceptable performance\n70 &lt; Accuracy &lt; 79: marginally acceptable performance\n80 &lt; Accuracy &lt; 89: acceptable performance\n90 &lt; Accuracy &lt; 99: excellent performance\nDo we need anything else regarding precision or recall? and does this all need to be a table? \n\n\n\nThere are four main measures of performance when evaluating a model:\n\nAccuracy\nPrecision\nRecall\nF1-Score\n\nEach of these performance measures has their own place in evaluating models, however, to begin to explain the differences between these models we should start with concepts of positive and negative outcomes.\n\nTrue Positive: predicted positive, was actually positive (correct)\nFalse Positive: predicted positive, was actually negative (incorrect)\nTrue Negative: predicted negative, was actually negative (correct)\nFalse Negative: predicted negative, was actually positive (incorrect)\n\nThese outcomes can be visualized on a confusion matrix. In the image below, green are correct predictions while red are incorrect predictions.\n\n\n\nconfusion_matrix\n\n\n\n3.2.1 Accuracy\nAccuracy is the ratio of correct predictions to all predictions. In other words, the total of the green squares divided by the entire matrix. This is arguably the most common concept of measuring performance.\n\\(Acccuracy = \\frac{TP+TN}{TP + TN + FN}\\)\n\n\n3.2.2 Precision\nPrecision is the ratio of true positives to the total number of positives (true positive + true negative).\n\\(Precision = \\frac{TP}{TP+FP}\\)\n\n\n3.2.3 Recall\nRecall is the ratio of true positives to the number of total correct predictions (true positive + false negative).\n\\(Recall = \\frac{TP}{TP+FN}\\)\n\n\n3.2.4 F1-Score\nF1-Score* is known as the harmonic mean between precision and recall. Precision and Recall are useful in their own rights, but the f1-Score is useful in the fact it’s a balanced combination of both precision and recall.\nF1-Score \\(= \\frac{2 * Precision * Recall}{Precision + Recall}\\)"
  },
  {
    "objectID": "methods.html#hypothesis-testing",
    "href": "methods.html#hypothesis-testing",
    "title": "3  Methods",
    "section": "3.3 Hypothesis Testing",
    "text": "3.3 Hypothesis Testing\nOur data consists of three main sets, the source input data, the Fairface output data, and the Deepface output data.\nWe’ll be creating our hypothesis tests by treating the source data as the basis for the original assumptions (our null hypotheses), and then using the output from Fairface and Deepface to test for statistically significant differences. Gaining a statistically significant result would allow us to reject our null hypothesis in favor of the alternative hypothesis. In other words, rejecting the original assumption means there is a statistically large enough difference between the source data and output data, and could indicate a bias in model.\nWe’ll be testing across different subsets contained within the data, as listed below:\n\n3.3.1 Demographics\n\nAge Group\nGender\nRace\n\n\n\n3.3.2 Demographics’ Subgroups\n\nAge Group (9 groups)\n\n0-2\n3-9\n10-19\n20-29\n30-39\n40-49\n50-59\n60-69\n70-130\n\nGender (2 groups)\n\nFemale\nMale\n\nRace (5 groups)\n\nAsian\nBlack\nIndian\nOther\nWhite\n\n\n\n\n3.3.3 The General Proportion Tests\nOur hypothesis tests will be testing different proportions within these subgroups between the source data and the output data.\nThe general format of our hypothesis tests will be:\n\\(H_0: p = p_{\\text{Source Data Subset}}\\)\n\\(H_A: p \\neq p_{\\text{Source Data Subset}}\\)\nWith the following test statistic:\n\\(\\frac{\\sqrt{n}(\\hat{p} - p)}{\\sqrt{p(1 - p)}}\\)\nWith the p-value being calculated by:\n\\(P(|Z| &gt; \\hat{p} | H_0)\\)\n\\(= P(|Z| &gt; \\frac{\\sqrt{n}(\\hat{p} - p)}{\\sqrt{p(1 - p)}})\\),\nwhere\n\n\\(n\\): output data subset size\n\\(\\hat{p}\\): output data subset proportion\n\\(p\\): source data subset proportion\n\n\nNOTE - may be worthwhile to state that we evaluated all cases here, but reduction to specific cases of controlling race and evaluating solely on gender or solely on race is of the most value to us. Performing tight filtering down to, for instance, the proportion of white people given they are 40-49 and female may be too restrictive.\nPerforming and analyzing calcuations of P(Age|Race) or P(Gender|Race) are of more interest, as I think our research questions are looking at disparate impact to the outcomes for each racial group.  ### Notation\nBefore we list the specific tests, we should introduce some notation.\nLet \\(R\\) be race, then \\(R \\in \\{Asian, Black, Indian, Other, White\\} = \\{A, B, I, O, W\\}\\)\nLet \\(G\\) be gender, then \\(G \\in \\{Female, Male\\} = \\{F, M\\}\\)\nLet \\(A\\) be age, then \\(A \\in \\{[0,2], [3,9], [10,19], [20,29], [30,39], [40,49], [50,59], [60,69], [70,130]\\} = \\{1, 2, 3, 4, 5, 6, 7, 8, 9\\}\\)\nLet \\(D\\) be the dataset, then \\(D \\in \\{Source, Fairface, Deepface\\} = \\{D_0, D_f, D_d\\}\\)\n\n\n3.3.4 More Specific Proportion Tests\nUsing this notation, we can simplify our nomenclature for testing a certain proportion of an overall demographic.\nFor example, we can test if the proportion of Female in the Fairface output is statistically different than the proportion of Female from the source.\nHypothesis Test:\n\\(H_0: p_F = p_{F|D_0}\\)\n\\(H_A: p_F \\neq p_{F|D_0}\\)\nP-value Calculation:\n\\(P(|Z| &gt; \\frac{\\sqrt{n}(\\hat{p} - p)}{\\sqrt{p(1 - p)}})\\),\nwhere\n\n\\(p = p_{F|D_0}\\): proportion of females from the source data\n\\(\\hat{p} = p_{F|D_f}\\): proportion of females from the fairface output\n\\(n = n_{F \\cup M|D_f}\\): number of data points in the gender subset form the fairface output\n\nAdditionally, we could test for different combinations of subsets within demographics. For instance, if we wanted to test for a statistically significant difference between the proportion of those who Female, given that they were Black, then we could write a hypothesis test like:\n\\(H_0: p_{F|B} = p_{F|D_0 \\cap B}\\)\n\\(H_A: p_{F|B} \\neq p_{F|D_0 \\cap B}\\)\nP-value Calculation:\n\\(P(|Z| &gt; \\frac{\\sqrt{n}(\\hat{p} - p)}{\\sqrt{p(1 - p)}})\\),\nwhere\n\n\\(p = p_{F|D_0 \\cap B}\\): proportion of females from the source data, given they were black\n\\(\\hat{p} = p_{F|D_f \\cap B}\\): proportion of females from the fairface output, given they were black\n\\(n = n_{F \\cup M|D_f \\cap B}\\): number of data points in the gender subset form the fairface output, given they were black.\n\nThese were two specific hypothesis tests, however, we’ll be testing many combinations of these parameters and reporting back on any significant findings.\n\n\n\n\n\n\nFrom the report requirements\n\n\n\nAlso can be called “Analyses”\nThis section might contain several subsections as needed.\n\nAt least one subsection should describe the exploratory data analysis you did.\nWhat modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)\nDescribe the analyses you did to answer the question of interest. Explain why you believe these methods are appropriate.\nAt least one subsection should describe the exploratory data analysis you did.\nWhat modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)\nDescribe the analyses you did to answer the question of interest. Explain why you believe these methods are appropriate.\nAt least one subsection should describe the exploratory data analysis you did.\nWhat modifications were necessary to make the dataset ready for analysis? (e.g. dealing with missing values, removing certain rows, replacing/cleaning text values, binning, etc)\nDescribe the analyses you did to answer the question of interest. Explain why you believe these methods are appropriate.\n\nSome methods we learn in this class include distribution comparison, correlation analysis, and hypothesis testing. You are required to include hypothesis tests into the project, but feel free to use additional methods to tell a good story about the data.\n\n\n\n\n\n\nKarkkainen, Kimmo, and Jungseock Joo. 2021. “FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation.” In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 1548–58."
  },
  {
    "objectID": "results.html#model-output",
    "href": "results.html#model-output",
    "title": "4  Results",
    "section": "4.1 Model Output",
    "text": "4.1 Model Output\nThe two models, DeepFace and FairFace, were run on the dataset described previously. In Figure 4.2, one can see the results of the predictions done by each model, by each factor that was considered: age, gender, and race. Note that the histogram distributions match the correct (source dataset) distributions, so we can see exactly the difference between what was provided and what was predicted, along with how well each model did on each category within each factor.\n\n\n\n\n\n\n\n(a) Gender predictions\n\n\n\n\n\n\n\n(b) Age predictions\n\n\n\n\n\n\n\n\n\n(c) Race predictions\n\n\n\n\nFigure 4.2: Histograms of the output from DeepFace and FairFace, with correct vs incorrect values colored. Note that the distributions match the correct (source dataset) distributions."
  },
  {
    "objectID": "results.html#model-performance",
    "href": "results.html#model-performance",
    "title": "4  Results",
    "section": "4.2 Model Performance",
    "text": "4.2 Model Performance\n\n4.2.1 TODO: Remove"
  },
  {
    "objectID": "results.html#hypothesis-testing",
    "href": "results.html#hypothesis-testing",
    "title": "4  Results",
    "section": "4.3 Hypothesis Testing",
    "text": "4.3 Hypothesis Testing\n\n4.3.1 TODO: Remove\n\n\n\n\n\n\nTest cateogry\nNull Proportion\nFairFace Proportion\nFairFace P-Value\nDeepFace Proportion\nDeepFace P-Value\n\n\n\n\n0-2\n0.0880010\n0.0762941\n0.0000031\n0.0000000\n0\n\n\n3-9\n0.0568832\n0.0664564\n0.0000137\n0.0000000\n0\n\n\n10-19\n0.0697867\n0.0606451\n0.0000522\n0.0206211\n0\n\n\n20-29\n0.3238735\n0.3480138\n0.0000000\n0.3987029\n0\n\n\n30-39\n0.1802755\n0.1762899\n0.2580216\n0.4047312\n0\n\n\n40-49\n0.0872542\n0.1023619\n0.0000000\n0.1467177\n0\n\n\n50-59\n0.0923160\n0.0930638\n0.7892078\n0.0268158\n0\n\n\n60-69\n0.0490831\n0.0490640\n1.0000000\n0.0024113\n0\n\n\n70-130\n0.0525268\n0.0278112\n0.0000000\n0.0000000\n0\n\n\n\n\n\n\n\n\n\n\n\n\nTest cateogry\nNull Proportion\nFairFace Proportion\nFairFace P-Value\nDeepFace Proportion\nDeepFace P-Value\n\n\n\n\nWhite\n0.4240727\n0.3854136\n0.00000\n0.3757120\n0\n\n\nBlack\n0.1891129\n0.1652484\n0.00000\n0.1479233\n0\n\n\nAsian\n0.1487843\n0.1448259\n0.22443\n0.2408847\n0\n\n\nIndian\n0.1670816\n0.1030675\n0.00000\n0.0611150\n0\n\n\nOther\n0.0709485\n0.2014445\n0.00000\n0.1743649\n0\n\n\n\n\n\n\n\n\n\n\n\n\nTest cateogry\nNull Proportion\nFairFace Proportion\nFairFace P-Value\nDeepFace Proportion\nDeepFace P-Value\n\n\n\n\nFemale\n0.4780101\n0.4797642\n0.7066925\n0.3833202\n0\n\n\nMale\n0.5219899\n0.5202358\n0.7066925\n0.6166798\n0\n\n\n\n\n\n\n\n\n\n\n\n\nTest Category\nTest Condition\nNull Proportion\nFairFace Proportion\nFairFace P-Value\nDeepFace Proportion\nDeepFace P-Value\n\n\n\n\nFemale\nWhite\n0.4572938\n0.4888530\n0.0000111\n0.4355428\n0.0025677\n\n\nFemale\nAsian\n0.5415505\n0.5431356\n0.9124653\n0.3879876\n0.0000000\n\n\nFemale\nBlack\n0.4872751\n0.4649586\n0.0415524\n0.2405846\n0.0000000\n\n\nFemale\nIndian\n0.4325801\n0.4430125\n0.4244901\n0.3496599\n0.0000000\n\n\nFemale\nOther\n0.5508772\n0.4477643\n0.0000000\n0.3972341\n0.0000000\n\n\nMale\nWhite\n0.5427062\n0.5111470\n0.0000111\n0.5644572\n0.0025677\n\n\nMale\nAsian\n0.4584495\n0.4568644\n0.9124653\n0.6120124\n0.0000000\n\n\nMale\nBlack\n0.5127249\n0.5350414\n0.0415524\n0.7594154\n0.0000000\n\n\nMale\nIndian\n0.5674199\n0.5569875\n0.4244901\n0.6503401\n0.0000000\n\n\nMale\nOther\n0.4491228\n0.5522357\n0.0000000\n0.6027659\n0.0000000\n\n\n\n\n\n\n\n\n\n4.3.2 Updated Table Version with Data from Carl, Bhav\n\n4.3.2.1 TODO: Remove\n\n\n\n\n\nRace\nCategory\nF1_d\nF1_f\nAccuracy_d\nAccuracy_f\nprop_d\nprop_f\np_value_d\np_value_f\n\n\n\n\nAll\n0-2\nNA\n0.8959757\n0.5000000\n0.9172888\n0.0000000\n0.0762941\n0.0000000\n0.0000031\n\n\nAll\n3-9\nNA\n0.7176035\n0.5000000\n0.8772778\n0.0000000\n0.0664564\n0.0000000\n0.0000137\n\n\nAll\n10-19\n0.0478601\n0.5052498\n0.5055825\n0.7211461\n0.0206211\n0.0606451\n0.0000000\n0.0000522\n\n\nAll\n20-29\n0.5054326\n0.7332922\n0.6217793\n0.8050592\n0.3987029\n0.3480138\n0.0000000\n0.0000000\n\n\nAll\n30-39\n0.3786318\n0.4670003\n0.6275447\n0.6741504\n0.4047312\n0.1762899\n0.0000000\n0.2580216\n\n\nAll\n40-49\n0.2276278\n0.3943970\n0.5866155\n0.6786302\n0.1467177\n0.1023619\n0.0000000\n0.0000000\n\n\nAll\n50-59\n0.0801673\n0.4633983\n0.5137145\n0.7049843\n0.0268158\n0.0930638\n0.0000000\n0.7892078\n\n\nAll\n60-69\n0.0016129\n0.3739425\n0.4991769\n0.6708204\n0.0024113\n0.0490640\n0.0000000\n1.0000000\n\n\nAll\n70-130\nNA\n0.6270661\n0.5000000\n0.7383514\n0.0000000\n0.0278112\n0.0000000\n0.0000000\n\n\nAll\nWhite\n0.8095461\n0.8610399\n0.8365916\n0.8788455\n0.3757120\n0.3854136\n0.0000000\n0.0000000\n\n\nAll\nBlack\n0.7964994\n0.8684858\n0.8462797\n0.8997692\n0.1479233\n0.1652484\n0.0000000\n0.0000000\n\n\nAll\nAsian\n0.7038975\n0.8948932\n0.9005150\n0.9338128\n0.2408847\n0.1448259\n0.0000000\n0.2244300\n\n\nAll\nIndian\n0.4092481\n0.6402458\n0.6310597\n0.7488102\n0.0611150\n0.1030675\n0.0000000\n0.0000000\n\n\nAll\nOther\n0.2389021\n0.3087473\n0.6283106\n0.7105889\n0.1743649\n0.2014445\n0.0000000\n0.0000000\n\n\nAll\nFemale\n0.8197702\n0.9429153\n0.8402892\n0.9453080\n0.3833202\n0.4797642\n0.0000000\n0.7066925\n\n\nAll\nMale\nNA\nNA\nNA\nNA\n0.6166798\n0.5202358\n0.0000000\n0.7066925\n\n\nWhite\n0-2\nNA\n0.9039010\n0.5000000\n0.9334307\n0.0000000\n0.0737749\n0.0000000\n0.0000000\n\n\nWhite\n3-9\nNA\n0.7503392\n0.5000000\n0.8668432\n0.0000000\n0.0770059\n0.0000000\n0.1757536\n\n\nWhite\n10-19\n0.0634648\n0.5638298\n0.5102546\n0.7330315\n0.0163771\n0.0693592\n0.0000000\n0.0000000\n\n\nWhite\n20-29\n0.4256326\n0.6697460\n0.6363584\n0.8072440\n0.3311940\n0.2455574\n0.0000000\n0.0000000\n\n\nWhite\n30-39\n0.3884765\n0.4731553\n0.6486930\n0.6821608\n0.4022353\n0.1628433\n0.0000000\n0.1406610\n\n\nWhite\n40-49\n0.2224248\n0.3847156\n0.5730236\n0.6683039\n0.2100255\n0.1186861\n0.0000000\n0.0001733\n\n\nWhite\n50-59\n0.0890599\n0.4832502\n0.5086819\n0.7046059\n0.0376231\n0.1419494\n0.0000000\n0.1140654\n\n\nWhite\n60-69\nNaN\n0.3545817\n0.4978778\n0.6482054\n0.0025451\n0.0680668\n0.0000000\n0.0469593\n\n\nWhite\n70-130\nNA\n0.6342183\n0.5000000\n0.7403000\n0.0000000\n0.0427571\n0.0000000\n0.0000000\n\n\nWhite\nMale\n0.8892356\n0.9595281\n0.8697687\n0.9566327\n0.5644572\n0.5111470\n0.0025677\n0.0000111\n\n\nWhite\nFemale\n0.8556585\n0.9526238\n0.8697687\n0.9566327\n0.4355428\n0.4888530\n0.0025677\n0.0000111\n\n\nAsian\n0-2\nNA\n0.9164589\n0.5000000\n0.9301909\n0.0000000\n0.2029235\n0.0000000\n0.0005397\n\n\nAsian\n3-9\nNA\n0.7140255\n0.5000000\n0.9111790\n0.0000000\n0.0928633\n0.0000000\n0.0000068\n\n\nAsian\n10-19\n0.0395257\n0.3798450\n0.5023622\n0.6944844\n0.0457370\n0.0366867\n0.0027191\n0.4219666\n\n\nAsian\n20-29\n0.5572885\n0.8557951\n0.5947638\n0.8792496\n0.5044874\n0.4379478\n0.0000000\n0.0065622\n\n\nAsian\n30-39\n0.2992611\n0.5069357\n0.6258649\n0.7042053\n0.3206766\n0.0988822\n0.0000000\n0.0016265\n\n\nAsian\n40-49\n0.1520190\n0.3320463\n0.5924407\n0.6626378\n0.0995858\n0.0369733\n0.0000000\n0.5439534\n\n\nAsian\n50-59\n0.0898876\n0.4608696\n0.5273304\n0.7272357\n0.0250259\n0.0315277\n0.0846407\n0.9976234\n\n\nAsian\n60-69\nNaN\n0.4141414\n0.4988555\n0.7581786\n0.0044874\n0.0315277\n0.0000000\n0.0082451\n\n\nAsian\n70-130\nNA\n0.7441860\n0.5000000\n0.8051278\n0.0000000\n0.0306678\n0.0000000\n0.0000205\n\n\nAsian\nMale\n0.7940330\n0.8914286\n0.7911354\n0.8993780\n0.6120124\n0.4568644\n0.0000000\n0.9124653\n\n\nAsian\nFemale\n0.7630232\n0.9058670\n0.7911354\n0.8993780\n0.3879876\n0.5431356\n0.0000000\n0.9124653\n\n\nBlack\n0-2\nNA\n0.8854962\n0.5000000\n0.9026663\n0.0000000\n0.0180859\n0.0000000\n0.4620785\n\n\nBlack\n3-9\nNA\n0.7400881\n0.5000000\n0.8957332\n0.0000000\n0.0298920\n0.0000000\n0.0562045\n\n\nBlack\n10-19\n0.0164609\n0.3784787\n0.5032694\n0.6953003\n0.0000000\n0.0635519\n0.0000000\n0.0152250\n\n\nBlack\n20-29\n0.5880567\n0.6875152\n0.6367203\n0.7184594\n0.4094997\n0.4687265\n0.1017071\n0.0001625\n\n\nBlack\n30-39\n0.4413203\n0.4518681\n0.5975793\n0.6299581\n0.4724564\n0.2398895\n0.0000000\n0.0058900\n\n\nBlack\n40-49\n0.1827542\n0.3260274\n0.5549318\n0.6301481\n0.1017426\n0.0864104\n0.0031364\n0.5412127\n\n\nBlack\n50-59\n0.0549451\n0.3565062\n0.5097384\n0.6523810\n0.0160202\n0.0557649\n0.0000000\n0.1387743\n\n\nBlack\n60-69\n0.0104712\n0.3508772\n0.5019317\n0.6526201\n0.0002811\n0.0301432\n0.0000000\n0.0149069\n\n\nBlack\n70-130\nNA\n0.4086022\n0.5000000\n0.6383490\n0.0000000\n0.0075358\n0.0000000\n0.0000000\n\n\nBlack\nMale\n0.8471279\n0.9637681\n0.8126869\n0.9625782\n0.7594154\n0.5350414\n0.0000000\n0.0415524\n\n\nBlack\nFemale\n0.7724665\n0.9615732\n0.8126869\n0.9625782\n0.2405846\n0.4649586\n0.0000000\n0.0415524\n\n\nIndian\n0-2\nNA\n0.8132911\n0.5000000\n0.8442303\n0.0000000\n0.0318164\n0.0000000\n0.0000000\n\n\nIndian\n3-9\nNA\n0.6047619\n0.5000000\n0.8704983\n0.0000000\n0.0535642\n0.0000000\n0.0187584\n\n\nIndian\n10-19\nNaN\n0.4719764\n0.4916753\n0.7130687\n0.0006803\n0.0543697\n0.0000000\n0.0792720\n\n\nIndian\n20-29\n0.4985183\n0.7635997\n0.5964419\n0.8073113\n0.3346939\n0.3407169\n0.0003270\n0.0001323\n\n\nIndian\n30-39\n0.3380175\n0.4403292\n0.5981267\n0.6658402\n0.5401361\n0.2219090\n0.0000000\n0.0000005\n\n\nIndian\n40-49\n0.2997904\n0.4636872\n0.6063907\n0.7199526\n0.1170068\n0.1494160\n0.7919834\n0.0000370\n\n\nIndian\n50-59\n0.0581655\n0.4649351\n0.5119443\n0.6957273\n0.0074830\n0.0841724\n0.0000000\n0.0337066\n\n\nIndian\n60-69\nNaN\n0.4563758\n0.4997424\n0.7281110\n0.0000000\n0.0447040\n0.0000000\n0.0645093\n\n\nIndian\n70-130\nNA\n0.5882353\n0.5000000\n0.7307264\n0.0000000\n0.0193315\n0.0000165\n0.0773759\n\n\nIndian\nMale\n0.8897860\n0.9574045\n0.8485028\n0.9525160\n0.6503401\n0.5569875\n0.0000000\n0.4244901\n\n\nIndian\nFemale\n0.8234153\n0.9452171\n0.8485028\n0.9525160\n0.3496599\n0.4430125\n0.0000000\n0.4244901\n\n\nOther\n0-2\nNA\n0.9193246\n0.5000000\n0.9396536\n0.0000000\n0.0605811\n0.0000000\n0.0000000\n\n\nOther\n3-9\nNA\n0.7043189\n0.5000000\n0.8634606\n0.0000000\n0.0638780\n0.0000000\n0.0165588\n\n\nOther\n10-19\n0.0597015\n0.5460317\n0.4968047\n0.7189416\n0.0195517\n0.0620235\n0.0000000\n0.0000000\n\n\nOther\n20-29\n0.4800507\n0.7610619\n0.5252573\n0.8012345\n0.4113019\n0.3840923\n0.4157510\n0.2760420\n\n\nOther\n30-39\n0.3505618\n0.5017182\n0.6391325\n0.7224076\n0.4213162\n0.1821554\n0.0000000\n0.0031518\n\n\nOther\n40-49\n0.3068783\n0.4630542\n0.6199385\n0.7061024\n0.1239866\n0.1071502\n0.0000000\n0.0000001\n\n\nOther\n50-59\n0.0983607\n0.5000000\n0.5278998\n0.7472707\n0.0219361\n0.0789203\n0.1499335\n0.0000000\n\n\nOther\n60-69\nNA\n0.6000000\n0.5000000\n0.8318627\n0.0019075\n0.0430661\n0.0256904\n0.0000000\n\n\nOther\n70-130\nNA\n0.5000000\n0.5000000\n0.6666667\n0.0000000\n0.0181331\n0.0007045\n0.0000206\n\n\nOther\nMale\n0.8216482\n0.9047310\n0.8341205\n0.9136183\n0.6027659\n0.5522357\n0.0000000\n0.0000000\n\n\nOther\nFemale\n0.8379888\n0.9216000\n0.8341205\n0.9136183\n0.3972341\n0.4477643\n0.0000000\n0.0000000\n\n\n\n\n\n\n\n# A tibble: 71 × 9\n   Race  Category source_prop   n_d  prop_d p_value_d   n_f prop_f p_value_f\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 All   0-2           0.0880 24053 0       0         24091 0.0763  3.11e- 6\n 2 All   3-9           0.0569 24053 0       9.20e-293 24091 0.0665  1.37e- 5\n 3 All   10-19         0.0698 24053 0.0206  2.52e-148 24091 0.0606  5.22e- 5\n 4 All   20-29         0.324  24053 0.399   2.00e- 65 24091 0.348   2.14e- 8\n 5 All   30-39         0.180  24053 0.405   0         24091 0.176   2.58e- 1\n 6 All   40-49         0.0873 24053 0.147   1.65e- 91 24091 0.102   1.65e- 8\n 7 All   50-59         0.0923 24053 0.0268  3.66e-202 24091 0.0931  7.89e- 1\n 8 All   60-69         0.0491 24053 0.00241 9.81e-229 24091 0.0491  1.00e+ 0\n 9 All   70-130        0.0525 24053 0       1.08e-283 24091 0.0278  2.83e-43\n10 All   White         0.424  24053 0.376   2.70e- 27 24091 0.385   5.83e-18\n# ℹ 61 more rows\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\nRemoved 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "results.html#model-performance-hypothesis-testing",
    "href": "results.html#model-performance-hypothesis-testing",
    "title": "4  Results",
    "section": "4.4 Model Performance, Hypothesis Testing",
    "text": "4.4 Model Performance, Hypothesis Testing\nFor each category and model, we calculate the F1 score, accuracy, and p-value, as described in section 3. The results are summarized in tbl-perf-pvalue. Cell values are colored according to the strength of the metric.\nWe also specifically looked at the performance metrics of the models, when controlled for specific race groups;\n\n4.4.1 TODO\n\nAdd color key\nColor p-values based on sig level = 99.7%\nBetter description of signifiance of numerical values of Accuracy, F1 score\nAdd line plot of F1, Accuracy, p-value OR correlation matrix\nMake table caption work?\n\nStatic table:\n\nValues where we FAIL to reject null hypothesis\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n [1] \"Race\"        \"Category\"    \"F1_d\"        \"F1_f\"        \"Accuracy_d\" \n [6] \"Accuracy_f\"  \"prop_d\"      \"prop_f\"      \"p_value_d\"   \"p_value_f\"  \n[11] \"n_f\"         \"source_prop\" \"n_d\"        \n\n\n\n\n\n4.4.2 Statistical Power\n\\[\n\\beta = P\\bigg(\\bigg|\\frac{\\sqrt{n}\\cdot\\hat{p}-p_a}{\\sqrt{p_a(1-p_a)}}\\bigg|\\geq\\frac{\\sqrt{n}\\cdot p_0-p_a}{\\sqrt{p_0(1-p_0)}}\\bigg)\n\\]\nOur selected level of significance is 99.7% (3-sigma). Type-II error is denoted by \\(\\beta\\) above, and Power will be \\(1-\\beta\\)\nWith \\(p_0\\) being our assumed population proportion (from the source dataset and what we used in our tests), \\(p_a\\) being the actual population proportion (from one or more of the below methods), \\(n\\) being the number of predicted members of a racial group (i.e. “Indian”),\n\nFor Gender - assume that sex at birth is a bernoulli trial, over time, the proportion for both genders should be 0.5\nFor age groups - assume that age has a true normal distribution. Each race may have different means and standard deviations for their distribution of age, but still adhere to a normal distribution. The “population” proportions may be a bit more challenging to calculate, but under this framework, we may be able to get there.\n\nMay be able to get via bootstrapping the source dataset, average age by race - I think that’s what we did in our last project?\nCould look at external data? May not have time to look through everything.\n\n\n\n\\[\n\\frac{\\sqrt{n_M}\\cdot(\\bar{p}_M-p_S)}{\\sqrt{p_S\\cdot(1-P_S)}}\n\\]"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "5  Conclusions",
    "section": "",
    "text": "From the report requirements\n\n\n\n\nSummarize what the paper has done, and discuss the implications of your Results.\nExplicitly connect the results to the research question.\nDiscuss how you would you extend this research\n\nLike the introduction, this section should be written with a non-expert in mind. A person should be able to read Introduction+Conclusion and get a rough idea of the meaning and significance of your paper"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Karkkainen, Kimmo, and Jungseock Joo. 2021. “FairFace: Face\nAttribute Dataset for Balanced Race, Gender, and Age for Bias\nMeasurement and Mitigation.” In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, 1548–58.\n\n\n“UTKFace.” 2021. UTKFace. https://susanqq.github.io/UTKFace."
  }
]